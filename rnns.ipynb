{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca1deee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "root = 'data_processed/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd1042a",
   "metadata": {},
   "source": [
    "# Generation with Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98c8a02",
   "metadata": {},
   "source": [
    "To establish a baseline of music generation that we can improve on, we use Recurrent Neural Networks. We formulate the problem as a next-note prediciton problem. This method is quite similar to  recurrence-based language models that are used in NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b5941b",
   "metadata": {},
   "source": [
    "The input is sequential, but unlike words in NLP, timing and dynamics (duration, velocity, offset) matter a lot in music. To be able to predict notes/chords + durations + offsets + velocities we might need multi-output heads (e.g., softmax for notes/chords/velocities, regression for durations/offsets)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8a953e",
   "metadata": {},
   "source": [
    "## Import Dataset and Definition of Useful functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f8ae12",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ff04479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_parse_all_columns_df(df):\n",
    "    \"\"\"\n",
    "    Parse all columns in a DataFrame to numeric, coercing errors.\n",
    "    \"\"\"\n",
    "    df['notes'] = df['notes'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    df['chords'] = df['chords'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    df['velocities'] = df['velocities'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    df['durations'] = df['durations'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    df['offsets'] = df['offsets'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    df['ordered_events'] = df['ordered_events'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    return df\n",
    "\n",
    "def load_dataframe_from_two_csvs(file1, file2):\n",
    "    \"\"\"\n",
    "    Load and concatenate two CSV files into a single pandas DataFrame.\n",
    "    \"\"\"\n",
    "    df1 = pd.read_csv(file1)\n",
    "    df2 = pd.read_csv(file2)\n",
    "    full_df = pd.concat([df1, df2], ignore_index=True)\n",
    "    full_df = safe_parse_all_columns_df(full_df)\n",
    "\n",
    "    return full_df\n",
    "\n",
    "def save_dataframe_to_two_csvs(df, file1, file2):\n",
    "    \"\"\"\n",
    "    Split a DataFrame in half and save it into two CSV files.\n",
    "    \"\"\"\n",
    "    halfway = len(df) // 2\n",
    "    df.iloc[:halfway].to_csv(file1, index=False)\n",
    "    df.iloc[halfway:].to_csv(file2, index=False)\n",
    "\n",
    "def load_dataframe_from_one_csv(file):\n",
    "    \"\"\"\n",
    "    Load a DataFrame from a single CSV file.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_dataframe_to_one_csv(df, file):\n",
    "    \"\"\"\n",
    "    Save a DataFrame to a single CSV file.\n",
    "    \"\"\"\n",
    "    df.to_csv(file, index=True)\n",
    "\n",
    "def load_reconstructed_events(file):\n",
    "    \"\"\"\n",
    "    Loads the reconstructed events CSV and safely parses the 'sequence' column,\n",
    "    converting notes to integers and chords to lists of integers.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    def safe_parse(seq_str):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(seq_str)\n",
    "            if not isinstance(parsed, list):\n",
    "                raise ValueError(\"Parsed sequence is not a list\")\n",
    "\n",
    "            normalized = []\n",
    "            for el in parsed:\n",
    "                if isinstance(el, list):\n",
    "                    normalized.append([int(x) for x in el])\n",
    "                else:\n",
    "                    normalized.append(int(el))\n",
    "            return normalized\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing sequence: {seq_str}\")\n",
    "            raise e\n",
    "\n",
    "    df['sequence'] = df['sequence'].apply(safe_parse)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d769e5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = root + 'data_part1.csv'\n",
    "file2 = root + 'data_part2.csv'\n",
    "\n",
    "df = load_dataframe_from_two_csvs(file1, file2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab83a2a",
   "metadata": {},
   "source": [
    "### Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd8d1336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_chord_to_list(chord):\n",
    "    \"\"\"\n",
    "    Convert a chord string to a list of integers.\n",
    "    \"\"\"\n",
    "    if isinstance(chord, str):\n",
    "        print([int(x) for x in chord.split(',') if x.isdigit()])\n",
    "        return [int(x) for x in chord.split(',') if x.isdigit()]\n",
    "    return []\n",
    "\n",
    "def reconstruct_ordered_events(df):\n",
    "    \"\"\"\n",
    "    Reconstruct the ordered list of events (notes and chords) for each song.\n",
    "    \"\"\"\n",
    "    sequences  = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        idx_note = 0\n",
    "        idx_chord = 0\n",
    "        reconstructed = []\n",
    "\n",
    "        for element in df['ordered_events'][i]:\n",
    "            if element == 'n':\n",
    "                reconstructed.append(df['notes'][i][idx_note])\n",
    "                idx_note += 1\n",
    "            elif element == 'c':\n",
    "                parsed_chord = parse_chord_to_list(df['chords'][i][idx_chord])\n",
    "                reconstructed.append(df['chords'][i][idx_chord])\n",
    "                idx_chord += 1\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown event type: {e}\")\n",
    "        \n",
    "        sequences.append(reconstructed)\n",
    "\n",
    "    reconstructed_dataset = pd.DataFrame({'sequence': sequences})\n",
    "    reconstructed_dataset.index.name = 'index'\n",
    "\n",
    "    return reconstructed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cca658",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataframe_to_one_csv(reconstruct_ordered_events(df), root + 'reconstructed_ordered_events.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467cf9b7",
   "metadata": {},
   "source": [
    "## Predict only Events (Notes and Chords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93a1e8d",
   "metadata": {},
   "source": [
    "### Creating the data: Fixed number of events "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1ace4b",
   "metadata": {},
   "source": [
    "Idea for creating the input sequences:\n",
    "- we take subsets of the list of events representing each song \n",
    "- we take the next event of each subset as corresponding training output sequences\n",
    "\n",
    "This is easy to implement and we will have a consistent sequence lenght for batching, but we are ignoring the timing aspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "619924df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, reconstructed_df):\n",
    "        \"\"\"\n",
    "        Build vocabulary of unique single notes only.\n",
    "        \"\"\"\n",
    "        self.notes = set()\n",
    "        for i in range(len(reconstructed_df)):\n",
    "            sequence = reconstructed_df['sequence'][i]\n",
    "            for event in sequence:\n",
    "                if isinstance(event, list):\n",
    "                    for note in event:\n",
    "                        self.notes.add(note)\n",
    "                else:\n",
    "                    self.notes.add(event)\n",
    "\n",
    "        self.notes = sorted(self.notes)\n",
    "        self.note_to_idx = {note: idx for idx, note in enumerate(self.notes)}\n",
    "        self.idx_to_note = {idx: note for idx, note in enumerate(self.notes)}\n",
    "        self.vocab_size = len(self.notes)\n",
    "\n",
    "    def encode_event(self, event):\n",
    "        \"\"\"\n",
    "        Encode an event as a multi-hot vector over single notes.\n",
    "        \"\"\"\n",
    "        vec = np.zeros(self.vocab_size, dtype=np.float32)\n",
    "        if isinstance(event, list):\n",
    "            for note in event:\n",
    "                vec[self.note_to_idx[note]] = 1.0\n",
    "        else:\n",
    "            vec[self.note_to_idx[event]] = 1.0\n",
    "        return vec\n",
    "\n",
    "    def decode_event(self, vec, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Decode multi-hot vector to list of notes.\n",
    "        \"\"\"\n",
    "        indices = np.where(vec >= threshold)[0]\n",
    "        notes = [self.idx_to_note[idx] for idx in indices]\n",
    "        if len(notes) == 1:\n",
    "            return notes[0]\n",
    "        else:\n",
    "            return notes\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.vocab_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8c06a4",
   "metadata": {},
   "source": [
    "Create Dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e5456a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicEventDataset(Dataset):\n",
    "    def __init__(self, reconstructed_df, vocab, seq_length=50):\n",
    "        \"\"\"\n",
    "        Constructs all valid (input_seq, target_event) pairs from each song in the dataset.\n",
    "\n",
    "        Args:\n",
    "            reconstructed_df: DataFrame with 'sequence' column where each entry is a list of events\n",
    "            vocab: Vocabulary object to encode events\n",
    "            seq_length: Length of each training input sequence (target is the next event)\n",
    "        \"\"\"\n",
    "        self.samples = []\n",
    "        self.seq_length = seq_length\n",
    "        self.vocab = vocab\n",
    "\n",
    "        for row_index in range(len(reconstructed_df)):\n",
    "            sequence = reconstructed_df['sequence'][row_index]\n",
    "            n_events = len(sequence)\n",
    "\n",
    "            if n_events <= seq_length:\n",
    "                continue\n",
    "\n",
    "            for i in range(n_events - seq_length):\n",
    "                input_seq = sequence[i : i + seq_length]\n",
    "                target_event = sequence[i + seq_length]\n",
    "                self.samples.append((input_seq, target_event))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seq, target_event = self.samples[idx]\n",
    "\n",
    "        input_encoded = np.array([self.vocab.encode_event(event) for event in input_seq], dtype=np.float32)\n",
    "        input_tensor = torch.tensor(input_encoded)\n",
    "\n",
    "        target_encoded = self.vocab.encode_event(target_event)\n",
    "        target_tensor = torch.tensor(target_encoded, dtype=torch.float32)\n",
    "\n",
    "        return input_tensor, target_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10a6155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence shape: torch.Size([16, 88])\n",
      "Next event shape: torch.Size([88])\n",
      "Input sequence (multi-hot vectors): tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Next event (multi-hot vector): tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "reconstructed_dataset = load_reconstructed_events(root + 'reconstructed_ordered_events.csv')\n",
    "\n",
    "vocab = Vocabulary(reconstructed_dataset)\n",
    "\n",
    "dataset = MusicEventDataset(reconstructed_dataset, vocab=vocab, seq_length=16)\n",
    "\n",
    "x, y = dataset[0]\n",
    "\n",
    "print(\"Input sequence shape:\", x.shape)\n",
    "print(\"Next event shape:\", y.shape)\n",
    "print(\"Input sequence (multi-hot vectors):\", x)\n",
    "print(\"Next event (multi-hot vector):\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13e88b8",
   "metadata": {},
   "source": [
    "### Model, Metrics and Cross-validation over sequence length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79868d61",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac442718",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicEventRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(MusicEventRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.rnn = nn.LSTM(input_size=input_size, hidden_size=hidden_size, \n",
    "                           num_layers=num_layers, batch_first=True)\n",
    "        \n",
    "        # Use output_size here explicitly\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        out, hidden = self.rnn(x, hidden)  \n",
    "        out_last = out[:, -1, :]\n",
    "        out = self.fc(out_last)\n",
    "\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ed761d",
   "metadata": {},
   "source": [
    "Cross-validation Loop on Sequence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cf93d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, epochs=10, batch_size=32, seq_length=16, val_split=0.1, print_every=5, checkpoint_path=\"best_model.pth\"):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    val_size = int(len(dataset) * val_split)\n",
    "    train_size = len(dataset) - val_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (x_batch, y_batch) in enumerate(train_loader, 1):\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(x_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if batch_idx % print_every == 0 or batch_idx == len(train_loader):\n",
    "                avg_loss = running_loss / batch_idx\n",
    "                print(f\"Epoch {epoch}, Batch {batch_idx}/{len(train_loader)}, Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x_val, y_val in val_loader:\n",
    "                x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "                outputs, _ = model(x_val)\n",
    "                loss = criterion(outputs, y_val)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch} completed, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"Validation loss improved. Model saved to {checkpoint_path}\")\n",
    "\n",
    "    print(\"Training done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33fe121a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_dataset = load_reconstructed_events(root + 'reconstructed_ordered_events.csv')\n",
    "vocab = Vocabulary(reconstructed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb4de14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code with hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55525567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_dataset, batch_size=128):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in val_loader:\n",
    "            x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "            outputs, _ = model(x_val)\n",
    "            loss = criterion(outputs, y_val)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    return val_loss / len(val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453049cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with seq_len=32, hidden_size=256, for 10 epochs...\n"
     ]
    }
   ],
   "source": [
    "seq_len = 32\n",
    "hidden_size = 256\n",
    "num_layers = 1\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "dataset = MusicEventDataset(reconstructed_dataset, vocab=vocab, seq_length=seq_len)\n",
    "\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "total_size = len(dataset)\n",
    "test_size = int(0.1 * total_size)\n",
    "train_size = total_size - test_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size], generator=generator)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = MusicEventRNN(\n",
    "    input_size=len(vocab),\n",
    "    hidden_size=hidden_size,\n",
    "    output_size=len(vocab),\n",
    "    num_layers=num_layers\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "print(f\"Training model with seq_len={seq_len}, hidden_size={hidden_size}, for {epochs} epochs...\")\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = model(x_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch}/{epochs} - Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"music_gen_seq32_hidden256.pth\")\n",
    "print(\"Model saved as 'music_gen_seq32_hidden256.pth'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1bbac4",
   "metadata": {},
   "source": [
    "## Predict Events + Durations + Offsets + Velocities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bd11a7",
   "metadata": {},
   "source": [
    "### Creating the data: Fixed time window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8752074a",
   "metadata": {},
   "source": [
    "The input is sequential, but unlike words in NLP, timing and dynamics (duration, velocity, offset) matter a lot in music. To be able to predict notes/chords + durations + offsets + velocities we might need multi-output heads (e.g., softmax for notes/chords/velocities, regression for durations/offsets)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e1ebed",
   "metadata": {},
   "source": [
    "This method respects the temporal structure of the music, but batching becomes trickier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
