{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca1deee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import random\n",
    "import time\n",
    "\n",
    "from music21 import stream, note, chord, midi, converter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "root = 'data_processed/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd1042a",
   "metadata": {},
   "source": [
    "# Generation with Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98c8a02",
   "metadata": {},
   "source": [
    "To establish a baseline of music generation that we can improve on, we use Recurrent Neural Networks. We formulate the problem as a next-note prediciton problem. This method is quite similar to  recurrence-based language models that are used in NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b5941b",
   "metadata": {},
   "source": [
    "The input is sequential, but unlike words in NLP, timing and dynamics (duration, velocity, offset) matter a lot in music. To be able to predict notes/chords + durations + offsets + velocities we might need multi-output heads (e.g., softmax for notes/chords/velocities, regression for durations/offsets)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8a953e",
   "metadata": {},
   "source": [
    "## Import Dataset and Definition of Useful functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f8ae12",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ff04479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_parse_all_columns_df(df):\n",
    "    \"\"\"\n",
    "    Parse all columns in a DataFrame to numeric, coercing errors.\n",
    "    \"\"\"\n",
    "    df['notes'] = df['notes'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    df['chords'] = df['chords'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    df['velocities'] = df['velocities'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    df['durations'] = df['durations'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    df['offsets'] = df['offsets'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    df['ordered_events'] = df['ordered_events'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    return df\n",
    "\n",
    "def load_dataframe_from_two_csvs(file1, file2):\n",
    "    \"\"\"\n",
    "    Load and concatenate two CSV files into a single pandas DataFrame.\n",
    "    \"\"\"\n",
    "    df1 = pd.read_csv(file1)\n",
    "    df2 = pd.read_csv(file2)\n",
    "    full_df = pd.concat([df1, df2], ignore_index=True)\n",
    "    full_df = safe_parse_all_columns_df(full_df)\n",
    "\n",
    "    return full_df\n",
    "\n",
    "def save_dataframe_to_two_csvs(df, file1, file2):\n",
    "    \"\"\"\n",
    "    Split a DataFrame in half and save it into two CSV files.\n",
    "    \"\"\"\n",
    "    halfway = len(df) // 2\n",
    "    df.iloc[:halfway].to_csv(file1, index=False)\n",
    "    df.iloc[halfway:].to_csv(file2, index=False)\n",
    "\n",
    "def load_dataframe_from_one_csv(file):\n",
    "    \"\"\"\n",
    "    Load a DataFrame from a single CSV file.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_dataframe_to_one_csv(df, file):\n",
    "    \"\"\"\n",
    "    Save a DataFrame to a single CSV file.\n",
    "    \"\"\"\n",
    "    df.to_csv(file, index=True)\n",
    "\n",
    "def load_reconstructed_events(file):\n",
    "    \"\"\"\n",
    "    Loads the reconstructed events CSV and safely parses the 'sequence' column,\n",
    "    converting notes to integers and chords to lists of integers.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    def safe_parse(seq_str):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(seq_str)\n",
    "            if not isinstance(parsed, list):\n",
    "                raise ValueError(\"Parsed sequence is not a list\")\n",
    "\n",
    "            normalized = []\n",
    "            for el in parsed:\n",
    "                if isinstance(el, list):\n",
    "                    normalized.append([int(x) for x in el])\n",
    "                else:\n",
    "                    normalized.append(int(el))\n",
    "            return normalized\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing sequence: {seq_str}\")\n",
    "            raise e\n",
    "\n",
    "    df['sequence'] = df['sequence'].apply(safe_parse)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d769e5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = root + 'data_part1.csv'\n",
    "file2 = root + 'data_part2.csv'\n",
    "\n",
    "df = load_dataframe_from_two_csvs(file1, file2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab83a2a",
   "metadata": {},
   "source": [
    "### Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd8d1336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_chord_to_list(chord):\n",
    "    \"\"\"\n",
    "    Convert a chord string to a list of integers.\n",
    "    \"\"\"\n",
    "    if isinstance(chord, str):\n",
    "        print([int(x) for x in chord.split(',') if x.isdigit()])\n",
    "        return [int(x) for x in chord.split(',') if x.isdigit()]\n",
    "    return []\n",
    "\n",
    "def reconstruct_ordered_events(df):\n",
    "    \"\"\"\n",
    "    Reconstruct the ordered list of events (notes and chords) for each song.\n",
    "    \"\"\"\n",
    "    sequences  = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        idx_note = 0\n",
    "        idx_chord = 0\n",
    "        reconstructed = []\n",
    "\n",
    "        for element in df['ordered_events'][i]:\n",
    "            if element == 'n':\n",
    "                reconstructed.append(df['notes'][i][idx_note])\n",
    "                idx_note += 1\n",
    "            elif element == 'c':\n",
    "                parsed_chord = parse_chord_to_list(df['chords'][i][idx_chord])\n",
    "                reconstructed.append(df['chords'][i][idx_chord])\n",
    "                idx_chord += 1\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown event type: {e}\")\n",
    "        \n",
    "        sequences.append(reconstructed)\n",
    "\n",
    "    reconstructed_dataset = pd.DataFrame({'sequence': sequences})\n",
    "    reconstructed_dataset.index.name = 'index'\n",
    "\n",
    "    return reconstructed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cca658",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataframe_to_one_csv(reconstruct_ordered_events(df), root + 'reconstructed_ordered_events.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467cf9b7",
   "metadata": {},
   "source": [
    "## Predict only Events (Notes and Chords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93a1e8d",
   "metadata": {},
   "source": [
    "### Creating the data: Fixed number of events "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1ace4b",
   "metadata": {},
   "source": [
    "Idea for creating the input sequences:\n",
    "- we take subsets of the list of events representing each song \n",
    "- we take the next event of each subset as corresponding training output sequences\n",
    "\n",
    "This is easy to implement and we will have a consistent sequence lenght for batching, but we are ignoring the timing aspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "619924df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, reconstructed_df):\n",
    "        \"\"\"\n",
    "        Build vocabulary of unique single notes only.\n",
    "        \"\"\"\n",
    "        self.notes = set()\n",
    "        for i in range(len(reconstructed_df)):\n",
    "            sequence = reconstructed_df['sequence'][i]\n",
    "            for event in sequence:\n",
    "                if isinstance(event, list):\n",
    "                    for note in event:\n",
    "                        self.notes.add(note)\n",
    "                else:\n",
    "                    self.notes.add(event)\n",
    "\n",
    "        self.notes = sorted(self.notes)\n",
    "        self.note_to_idx = {note: idx for idx, note in enumerate(self.notes)}\n",
    "        self.idx_to_note = {idx: note for idx, note in enumerate(self.notes)}\n",
    "        self.vocab_size = len(self.notes)\n",
    "\n",
    "    def encode_event(self, event):\n",
    "        \"\"\"\n",
    "        Encode an event as a multi-hot vector over single notes.\n",
    "        \"\"\"\n",
    "        vec = np.zeros(self.vocab_size, dtype=np.float32)\n",
    "        if isinstance(event, list):\n",
    "            for note in event:\n",
    "                vec[self.note_to_idx[note]] = 1.0\n",
    "        else:\n",
    "            vec[self.note_to_idx[event]] = 1.0\n",
    "        return vec\n",
    "\n",
    "    def decode_event(self, vec, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Decode multi-hot vector to list of notes.\n",
    "        \"\"\"\n",
    "        indices = np.where(vec >= threshold)[0]\n",
    "        notes = [self.idx_to_note[idx] for idx in indices]\n",
    "        if len(notes) == 1:\n",
    "            return notes[0]\n",
    "        else:\n",
    "            return notes\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.vocab_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8c06a4",
   "metadata": {},
   "source": [
    "Create Dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e5456a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicEventDataset(Dataset):\n",
    "    def __init__(self, reconstructed_df, vocab, seq_length=50):\n",
    "        \"\"\"\n",
    "        Constructs all valid (input_seq, target_event) pairs from each song in the dataset.\n",
    "\n",
    "        Args:\n",
    "            reconstructed_df: DataFrame with 'sequence' column where each entry is a list of events\n",
    "            vocab: Vocabulary object to encode events\n",
    "            seq_length: Length of each training input sequence (target is the next event)\n",
    "        \"\"\"\n",
    "        self.samples = []\n",
    "        self.seq_length = seq_length\n",
    "        self.vocab = vocab\n",
    "\n",
    "        for row_index in range(len(reconstructed_df)):\n",
    "            sequence = reconstructed_df['sequence'][row_index]\n",
    "            n_events = len(sequence)\n",
    "\n",
    "            if n_events <= seq_length:\n",
    "                continue\n",
    "\n",
    "            for i in range(n_events - seq_length):\n",
    "                input_seq = sequence[i : i + seq_length]\n",
    "                target_event = sequence[i + seq_length]\n",
    "                self.samples.append((input_seq, target_event))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seq, target_event = self.samples[idx]\n",
    "\n",
    "        input_encoded = np.array([self.vocab.encode_event(event) for event in input_seq], dtype=np.float32)\n",
    "        input_tensor = torch.tensor(input_encoded)\n",
    "\n",
    "        target_encoded = self.vocab.encode_event(target_event)\n",
    "        target_tensor = torch.tensor(target_encoded, dtype=torch.float32)\n",
    "\n",
    "        return input_tensor, target_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b10a6155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence shape: torch.Size([16, 88])\n",
      "Next event shape: torch.Size([88])\n",
      "Input sequence (multi-hot vectors): tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Next event (multi-hot vector): tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "reconstructed_dataset = load_reconstructed_events(root + 'reconstructed_ordered_events.csv')\n",
    "\n",
    "vocab = Vocabulary(reconstructed_dataset)\n",
    "\n",
    "dataset = MusicEventDataset(reconstructed_dataset, vocab=vocab, seq_length=16)\n",
    "\n",
    "x, y = dataset[0]\n",
    "\n",
    "print(\"Input sequence shape:\", x.shape)\n",
    "print(\"Next event shape:\", y.shape)\n",
    "print(\"Input sequence (multi-hot vectors):\", x)\n",
    "print(\"Next event (multi-hot vector):\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13e88b8",
   "metadata": {},
   "source": [
    "### Model, Metrics and Cross-validation over sequence length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79868d61",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac442718",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicEventRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(MusicEventRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.rnn = nn.LSTM(input_size=input_size, hidden_size=hidden_size, \n",
    "                           num_layers=num_layers, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        out, hidden = self.rnn(x, hidden)  \n",
    "        out_last = out[:, -1, :]\n",
    "        out = self.fc(out_last)\n",
    "\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ed761d",
   "metadata": {},
   "source": [
    "Cross-validation Loop on Sequence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33fe121a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_dataset = load_reconstructed_events(root + 'reconstructed_ordered_events.csv')\n",
    "vocab = Vocabulary(reconstructed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ab6518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, epochs=10, batch_size=32, seq_length=16, val_split=0.1, print_every=5, checkpoint_path=\"best_model.pth\"):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (x_batch, y_batch) in enumerate(train_loader, 1):\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(x_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if batch_idx % print_every == 0 or batch_idx == len(train_loader):\n",
    "                avg_loss = running_loss / batch_idx\n",
    "                print(f\"Epoch {epoch}, Batch {batch_idx}/{len(train_loader)}, Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(\"Training done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71074343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_dataset, batch_size=128):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in val_loader:\n",
    "            x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "            outputs, _ = model(x_val)\n",
    "            loss = criterion(outputs, y_val)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    return val_loss / len(val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca68c004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Tuning for sequence length: 8 ===\n",
      "\n",
      "-- hidden_size=64, num_layers=1 --\n",
      "  Fold 1/5\n",
      "Using device: cpu\n",
      "Epoch 1, Batch 10000/25282, Avg Loss: 0.0830\n",
      "Epoch 1, Batch 20000/25282, Avg Loss: 0.0800\n",
      "Epoch 1, Batch 25282/25282, Avg Loss: 0.0792\n",
      "Epoch 2, Batch 10000/25282, Avg Loss: 0.0762\n",
      "Epoch 2, Batch 20000/25282, Avg Loss: 0.0762\n",
      "Epoch 2, Batch 25282/25282, Avg Loss: 0.0761\n",
      "Epoch 3, Batch 10000/25282, Avg Loss: 0.0759\n",
      "Epoch 3, Batch 20000/25282, Avg Loss: 0.0759\n",
      "Epoch 3, Batch 25282/25282, Avg Loss: 0.0759\n",
      "Training done.\n",
      "    -> Validation Loss: 0.0758\n",
      "  Fold 2/5\n",
      "Using device: cpu\n",
      "Epoch 1, Batch 10000/25282, Avg Loss: 0.0830\n",
      "Epoch 1, Batch 20000/25282, Avg Loss: 0.0799\n",
      "Epoch 1, Batch 25282/25282, Avg Loss: 0.0792\n",
      "Epoch 2, Batch 10000/25282, Avg Loss: 0.0762\n",
      "Epoch 2, Batch 20000/25282, Avg Loss: 0.0761\n",
      "Epoch 2, Batch 25282/25282, Avg Loss: 0.0761\n",
      "Epoch 3, Batch 10000/25282, Avg Loss: 0.0759\n",
      "Epoch 3, Batch 20000/25282, Avg Loss: 0.0759\n",
      "Epoch 3, Batch 25282/25282, Avg Loss: 0.0758\n",
      "Training done.\n",
      "    -> Validation Loss: 0.0758\n",
      "  Fold 3/5\n",
      "Using device: cpu\n",
      "Epoch 1, Batch 10000/25282, Avg Loss: 0.0829\n",
      "Epoch 1, Batch 20000/25282, Avg Loss: 0.0799\n",
      "Epoch 1, Batch 25282/25282, Avg Loss: 0.0792\n",
      "Epoch 2, Batch 10000/25282, Avg Loss: 0.0762\n",
      "Epoch 2, Batch 20000/25282, Avg Loss: 0.0761\n",
      "Epoch 2, Batch 25282/25282, Avg Loss: 0.0761\n",
      "Epoch 3, Batch 10000/25282, Avg Loss: 0.0759\n",
      "Epoch 3, Batch 20000/25282, Avg Loss: 0.0759\n",
      "Epoch 3, Batch 25282/25282, Avg Loss: 0.0758\n",
      "Training done.\n",
      "    -> Validation Loss: 0.0759\n",
      "  Fold 4/5\n",
      "Using device: cpu\n",
      "Epoch 1, Batch 10000/25282, Avg Loss: 0.0828\n",
      "Epoch 1, Batch 20000/25282, Avg Loss: 0.0799\n",
      "Epoch 1, Batch 25282/25282, Avg Loss: 0.0791\n",
      "Epoch 2, Batch 10000/25282, Avg Loss: 0.0762\n",
      "Epoch 2, Batch 20000/25282, Avg Loss: 0.0761\n",
      "Epoch 2, Batch 25282/25282, Avg Loss: 0.0761\n",
      "Epoch 3, Batch 10000/25282, Avg Loss: 0.0759\n",
      "Epoch 3, Batch 20000/25282, Avg Loss: 0.0759\n",
      "Epoch 3, Batch 25282/25282, Avg Loss: 0.0758\n",
      "Training done.\n",
      "    -> Validation Loss: 0.0757\n",
      "  Fold 5/5\n",
      "Using device: cpu\n",
      "Epoch 1, Batch 10000/25282, Avg Loss: 0.0829\n",
      "Epoch 1, Batch 20000/25282, Avg Loss: 0.0799\n",
      "Epoch 1, Batch 25282/25282, Avg Loss: 0.0792\n",
      "Epoch 2, Batch 10000/25282, Avg Loss: 0.0762\n",
      "Epoch 2, Batch 20000/25282, Avg Loss: 0.0761\n",
      "Epoch 2, Batch 25282/25282, Avg Loss: 0.0761\n",
      "Epoch 3, Batch 10000/25282, Avg Loss: 0.0758\n",
      "Epoch 3, Batch 20000/25282, Avg Loss: 0.0758\n",
      "Epoch 3, Batch 25282/25282, Avg Loss: 0.0758\n",
      "Training done.\n",
      "    -> Validation Loss: 0.0758\n",
      "\n",
      "-- hidden_size=128, num_layers=1 --\n",
      "  Fold 1/5\n",
      "Using device: cpu\n",
      "Epoch 1, Batch 10000/25282, Avg Loss: 0.0805\n",
      "Epoch 1, Batch 20000/25282, Avg Loss: 0.0785\n",
      "Epoch 1, Batch 25282/25282, Avg Loss: 0.0780\n",
      "Epoch 2, Batch 10000/25282, Avg Loss: 0.0760\n",
      "Epoch 2, Batch 20000/25282, Avg Loss: 0.0759\n",
      "Epoch 2, Batch 25282/25282, Avg Loss: 0.0758\n",
      "Epoch 3, Batch 10000/25282, Avg Loss: 0.0756\n",
      "Epoch 3, Batch 20000/25282, Avg Loss: 0.0756\n",
      "Epoch 3, Batch 25282/25282, Avg Loss: 0.0755\n",
      "Training done.\n",
      "    -> Validation Loss: 0.0755\n",
      "  Fold 2/5\n",
      "Using device: cpu\n",
      "Epoch 1, Batch 10000/25282, Avg Loss: 0.0806\n",
      "Epoch 1, Batch 20000/25282, Avg Loss: 0.0785\n",
      "Epoch 1, Batch 25282/25282, Avg Loss: 0.0781\n",
      "Epoch 2, Batch 10000/25282, Avg Loss: 0.0760\n",
      "Epoch 2, Batch 20000/25282, Avg Loss: 0.0758\n",
      "Epoch 2, Batch 25282/25282, Avg Loss: 0.0758\n",
      "Epoch 3, Batch 10000/25282, Avg Loss: 0.0755\n",
      "Epoch 3, Batch 20000/25282, Avg Loss: 0.0755\n",
      "Epoch 3, Batch 25282/25282, Avg Loss: 0.0755\n",
      "Training done.\n",
      "    -> Validation Loss: 0.0756\n",
      "  Fold 3/5\n",
      "Using device: cpu\n",
      "Epoch 1, Batch 10000/25282, Avg Loss: 0.0808\n",
      "Epoch 1, Batch 20000/25282, Avg Loss: 0.0787\n",
      "Epoch 1, Batch 25282/25282, Avg Loss: 0.0781\n",
      "Epoch 2, Batch 10000/25282, Avg Loss: 0.0759\n",
      "Epoch 2, Batch 20000/25282, Avg Loss: 0.0759\n",
      "Epoch 2, Batch 25282/25282, Avg Loss: 0.0758\n",
      "Epoch 3, Batch 10000/25282, Avg Loss: 0.0756\n",
      "Epoch 3, Batch 20000/25282, Avg Loss: 0.0755\n",
      "Epoch 3, Batch 25282/25282, Avg Loss: 0.0755\n",
      "Training done.\n",
      "    -> Validation Loss: 0.0756\n",
      "  Fold 4/5\n",
      "Using device: cpu\n",
      "Epoch 1, Batch 10000/25282, Avg Loss: 0.0807\n",
      "Epoch 1, Batch 20000/25282, Avg Loss: 0.0786\n",
      "Epoch 1, Batch 25282/25282, Avg Loss: 0.0781\n",
      "Epoch 2, Batch 10000/25282, Avg Loss: 0.0759\n",
      "Epoch 2, Batch 20000/25282, Avg Loss: 0.0759\n",
      "Epoch 2, Batch 25282/25282, Avg Loss: 0.0758\n",
      "Epoch 3, Batch 10000/25282, Avg Loss: 0.0755\n",
      "Epoch 3, Batch 20000/25282, Avg Loss: 0.0755\n",
      "Epoch 3, Batch 25282/25282, Avg Loss: 0.0755\n",
      "Training done.\n",
      "    -> Validation Loss: 0.0755\n",
      "  Fold 5/5\n",
      "Using device: cpu\n",
      "Epoch 1, Batch 10000/25282, Avg Loss: 0.0805\n",
      "Epoch 1, Batch 20000/25282, Avg Loss: 0.0785\n",
      "Epoch 1, Batch 25282/25282, Avg Loss: 0.0780\n",
      "Epoch 2, Batch 10000/25282, Avg Loss: 0.0759\n",
      "Epoch 2, Batch 20000/25282, Avg Loss: 0.0758\n",
      "Epoch 2, Batch 25282/25282, Avg Loss: 0.0758\n",
      "Epoch 3, Batch 10000/25282, Avg Loss: 0.0755\n",
      "Epoch 3, Batch 20000/25282, Avg Loss: 0.0755\n",
      "Epoch 3, Batch 25282/25282, Avg Loss: 0.0755\n",
      "Training done.\n",
      "    -> Validation Loss: 0.0755\n",
      "\n",
      "-- hidden_size=256, num_layers=1 --\n",
      "  Fold 1/5\n",
      "Using device: cpu\n",
      "Epoch 1, Batch 10000/25282, Avg Loss: 0.0795\n",
      "Epoch 1, Batch 20000/25282, Avg Loss: 0.0779\n",
      "Epoch 1, Batch 25282/25282, Avg Loss: 0.0775\n",
      "Epoch 2, Batch 10000/25282, Avg Loss: 0.0757\n",
      "Epoch 2, Batch 20000/25282, Avg Loss: 0.0756\n",
      "Epoch 2, Batch 25282/25282, Avg Loss: 0.0756\n",
      "Epoch 3, Batch 10000/25282, Avg Loss: 0.0752\n",
      "Epoch 3, Batch 20000/25282, Avg Loss: 0.0752\n",
      "Epoch 3, Batch 25282/25282, Avg Loss: 0.0752\n",
      "Training done.\n",
      "    -> Validation Loss: 0.0751\n",
      "  Fold 2/5\n",
      "Using device: cpu\n",
      "Epoch 1, Batch 10000/25282, Avg Loss: 0.0793\n",
      "Epoch 1, Batch 20000/25282, Avg Loss: 0.0778\n",
      "Epoch 1, Batch 25282/25282, Avg Loss: 0.0774\n",
      "Epoch 2, Batch 10000/25282, Avg Loss: 0.0757\n",
      "Epoch 2, Batch 20000/25282, Avg Loss: 0.0756\n",
      "Epoch 2, Batch 25282/25282, Avg Loss: 0.0756\n",
      "Epoch 3, Batch 10000/25282, Avg Loss: 0.0752\n",
      "Epoch 3, Batch 20000/25282, Avg Loss: 0.0752\n",
      "Epoch 3, Batch 25282/25282, Avg Loss: 0.0752\n",
      "Training done.\n",
      "    -> Validation Loss: 0.0752\n",
      "  Fold 3/5\n",
      "Using device: cpu\n",
      "Epoch 1, Batch 10000/25282, Avg Loss: 0.0794\n",
      "Epoch 1, Batch 20000/25282, Avg Loss: 0.0778\n",
      "Epoch 1, Batch 25282/25282, Avg Loss: 0.0774\n",
      "Epoch 2, Batch 10000/25282, Avg Loss: 0.0757\n",
      "Epoch 2, Batch 20000/25282, Avg Loss: 0.0756\n",
      "Epoch 2, Batch 25282/25282, Avg Loss: 0.0756\n",
      "Epoch 3, Batch 10000/25282, Avg Loss: 0.0752\n",
      "Epoch 3, Batch 20000/25282, Avg Loss: 0.0752\n",
      "Epoch 3, Batch 25282/25282, Avg Loss: 0.0752\n",
      "Training done.\n",
      "    -> Validation Loss: 0.0753\n",
      "  Fold 4/5\n",
      "Using device: cpu\n",
      "Epoch 1, Batch 10000/25282, Avg Loss: 0.0795\n",
      "Epoch 1, Batch 20000/25282, Avg Loss: 0.0779\n",
      "Epoch 1, Batch 25282/25282, Avg Loss: 0.0775\n",
      "Epoch 2, Batch 10000/25282, Avg Loss: 0.0757\n",
      "Epoch 2, Batch 20000/25282, Avg Loss: 0.0756\n",
      "Epoch 2, Batch 25282/25282, Avg Loss: 0.0756\n",
      "Epoch 3, Batch 10000/25282, Avg Loss: 0.0752\n",
      "Epoch 3, Batch 20000/25282, Avg Loss: 0.0752\n",
      "Epoch 3, Batch 25282/25282, Avg Loss: 0.0752\n",
      "Training done.\n",
      "    -> Validation Loss: 0.0751\n",
      "  Fold 5/5\n",
      "Using device: cpu\n",
      "Epoch 1, Batch 10000/25282, Avg Loss: 0.0795\n",
      "Epoch 1, Batch 20000/25282, Avg Loss: 0.0779\n",
      "Epoch 1, Batch 25282/25282, Avg Loss: 0.0775\n",
      "Epoch 2, Batch 10000/25282, Avg Loss: 0.0757\n",
      "Epoch 2, Batch 20000/25282, Avg Loss: 0.0756\n",
      "Epoch 2, Batch 25282/25282, Avg Loss: 0.0756\n",
      "Epoch 3, Batch 10000/25282, Avg Loss: 0.0752\n",
      "Epoch 3, Batch 20000/25282, Avg Loss: 0.0752\n",
      "Epoch 3, Batch 25282/25282, Avg Loss: 0.0752\n",
      "Training done.\n",
      "    -> Validation Loss: 0.0753\n",
      "\n",
      "=== Tuning for sequence length: 16 ===\n",
      "\n",
      "-- hidden_size=64, num_layers=1 --\n",
      "  Fold 1/5\n",
      "Using device: cpu\n",
      "Epoch 1, Batch 10000/25157, Avg Loss: 0.0823\n",
      "Epoch 1, Batch 20000/25157, Avg Loss: 0.0791\n",
      "Epoch 1, Batch 25157/25157, Avg Loss: 0.0783\n",
      "Epoch 2, Batch 10000/25157, Avg Loss: 0.0752\n",
      "Epoch 2, Batch 20000/25157, Avg Loss: 0.0751\n",
      "Epoch 2, Batch 25157/25157, Avg Loss: 0.0751\n",
      "Epoch 3, Batch 10000/25157, Avg Loss: 0.0748\n",
      "Epoch 3, Batch 20000/25157, Avg Loss: 0.0747\n",
      "Epoch 3, Batch 25157/25157, Avg Loss: 0.0747\n",
      "Training done.\n",
      "    -> Validation Loss: 0.0746\n",
      "  Fold 2/5\n",
      "Using device: cpu\n",
      "Epoch 1, Batch 10000/25157, Avg Loss: 0.0820\n",
      "Epoch 1, Batch 20000/25157, Avg Loss: 0.0789\n",
      "Epoch 1, Batch 25157/25157, Avg Loss: 0.0783\n",
      "Epoch 2, Batch 10000/25157, Avg Loss: 0.0752\n",
      "Epoch 2, Batch 20000/25157, Avg Loss: 0.0751\n",
      "Epoch 2, Batch 25157/25157, Avg Loss: 0.0751\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     28\u001b[39m val_subset = Subset(train_dataset, val_idx)\n\u001b[32m     30\u001b[39m model = MusicEventRNN(\n\u001b[32m     31\u001b[39m     input_size=\u001b[38;5;28mlen\u001b[39m(vocab),\n\u001b[32m     32\u001b[39m     hidden_size=hidden_size,\n\u001b[32m     33\u001b[39m     output_size=\u001b[38;5;28mlen\u001b[39m(vocab),\n\u001b[32m     34\u001b[39m     num_layers=num_layers\n\u001b[32m     35\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_subset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Will not be used\u001b[39;49;00m\n\u001b[32m     45\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m val_loss = evaluate_model(model, val_subset, batch_size=\u001b[32m128\u001b[39m)\n\u001b[32m     48\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m    -> Validation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, dataset, epochs, batch_size, seq_length, val_split, print_every, checkpoint_path)\u001b[39m\n\u001b[32m     15\u001b[39m x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n\u001b[32m     17\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m outputs, _ = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m loss = criterion(outputs, y_batch)\n\u001b[32m     20\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Program Files\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Program Files\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mMusicEventRNN.forward\u001b[39m\u001b[34m(self, x, hidden)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, hidden=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     out, hidden = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[32m     15\u001b[39m     out_last = out[:, -\u001b[32m1\u001b[39m, :]\n\u001b[32m     16\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.fc(out_last)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Program Files\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Program Files\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Program Files\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1124\u001b[39m, in \u001b[36mLSTM.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m   1121\u001b[39m         hx = \u001b[38;5;28mself\u001b[39m.permute_hidden(hx, sorted_indices)\n\u001b[32m   1123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1124\u001b[39m     result = \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1125\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1126\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1127\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1128\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1129\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1130\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1131\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1136\u001b[39m     result = _VF.lstm(\n\u001b[32m   1137\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1138\u001b[39m         batch_sizes,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1145\u001b[39m         \u001b[38;5;28mself\u001b[39m.bidirectional,\n\u001b[32m   1146\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "sequence_lengths = [8, 16, 32]\n",
    "hidden_sizes = [64, 128, 256]\n",
    "num_layers_options = [1]\n",
    "k_folds = 5\n",
    "results = {}\n",
    "\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "\n",
    "for seq_len in sequence_lengths:\n",
    "    print(f\"\\n=== Tuning for sequence length: {seq_len} ===\")\n",
    "    dataset = MusicEventDataset(reconstructed_dataset, vocab=vocab, seq_length=seq_len)\n",
    "\n",
    "    total_size = len(dataset)\n",
    "    test_size = int(0.1 * total_size)\n",
    "    train_size = total_size - test_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size], generator=generator)\n",
    "\n",
    "    for hidden_size in hidden_sizes:\n",
    "        for num_layers in num_layers_options:\n",
    "            fold_losses = []\n",
    "            print(f\"\\n-- hidden_size={hidden_size}, num_layers={num_layers} --\")\n",
    "\n",
    "            kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "            for fold, (train_idx, val_idx) in enumerate(kf.split(train_dataset)):\n",
    "                print(f\"  Fold {fold+1}/{k_folds}\")\n",
    "                train_subset = Subset(train_dataset, train_idx)\n",
    "                val_subset = Subset(train_dataset, val_idx)\n",
    "\n",
    "                model = MusicEventRNN(\n",
    "                    input_size=len(vocab),\n",
    "                    hidden_size=hidden_size,\n",
    "                    output_size=len(vocab),\n",
    "                    num_layers=num_layers\n",
    "                )\n",
    "\n",
    "                train_model(\n",
    "                    model=model,\n",
    "                    dataset=train_subset,\n",
    "                    epochs=3,\n",
    "                    batch_size=128,\n",
    "                    seq_length=seq_len,\n",
    "                    print_every=10000,\n",
    "                    checkpoint_path=None\n",
    "                )\n",
    "\n",
    "                val_loss = evaluate_model(model, val_subset, batch_size=128)\n",
    "                print(f\"    -> Validation Loss: {val_loss:.4f}\")\n",
    "                fold_losses.append(val_loss)\n",
    "\n",
    "            avg_loss = sum(fold_losses) / len(fold_losses)\n",
    "            results[(seq_len, hidden_size, num_layers)] = avg_loss\n",
    "\n",
    "print(\"\\n=== K-Fold Tuning Results ===\")\n",
    "for (seq_len, hidden_size, num_layers), val_loss in results.items():\n",
    "    print(f\"Seq Len: {seq_len}, Hidden: {hidden_size}, Layers: {num_layers} => Avg Val Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621299b5",
   "metadata": {},
   "source": [
    "The best model we have found is the one with sequence length 32 and hidden layer size 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966fac29",
   "metadata": {},
   "source": [
    "### Training and Music Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "453049cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuing training for 5 more epochs with seq_len=32, hidden_size=256...\n",
      "Epoch 1, Batch 10000/31134 - Avg Loss So Far: 0.0727\n",
      "Epoch 1, Batch 20000/31134 - Avg Loss So Far: 0.0728\n",
      "Epoch 1, Batch 30000/31134 - Avg Loss So Far: 0.0728\n",
      "Epoch 1/5 - Avg Loss: 0.0728\n",
      "Epoch 2, Batch 10000/31134 - Avg Loss So Far: 0.0726\n",
      "Epoch 2, Batch 20000/31134 - Avg Loss So Far: 0.0726\n",
      "Epoch 2, Batch 30000/31134 - Avg Loss So Far: 0.0726\n",
      "Epoch 2/5 - Avg Loss: 0.0726\n",
      "Epoch 3, Batch 10000/31134 - Avg Loss So Far: 0.0725\n",
      "Epoch 3, Batch 20000/31134 - Avg Loss So Far: 0.0725\n",
      "Epoch 3, Batch 30000/31134 - Avg Loss So Far: 0.0725\n",
      "Epoch 3/5 - Avg Loss: 0.0725\n",
      "Epoch 4, Batch 10000/31134 - Avg Loss So Far: 0.0724\n",
      "Epoch 4, Batch 20000/31134 - Avg Loss So Far: 0.0724\n",
      "Epoch 4, Batch 30000/31134 - Avg Loss So Far: 0.0724\n",
      "Epoch 4/5 - Avg Loss: 0.0725\n",
      "Epoch 5, Batch 10000/31134 - Avg Loss So Far: 0.0723\n",
      "Epoch 5, Batch 20000/31134 - Avg Loss So Far: 0.0723\n",
      "Epoch 5, Batch 30000/31134 - Avg Loss So Far: 0.0724\n",
      "Epoch 5/5 - Avg Loss: 0.0724\n",
      "Model saved as 'music_gen_seq32_hidden256.pth'\n"
     ]
    }
   ],
   "source": [
    "seq_len = 32\n",
    "hidden_size = 256\n",
    "num_layers = 1\n",
    "batch_size = 128\n",
    "epochs = 5\n",
    "learning_rate = 0.001\n",
    "checkpoint_path = \"music_gen_seq32_hidden256.pth\"\n",
    "\n",
    "dataset = MusicEventDataset(reconstructed_dataset, vocab=vocab, seq_length=seq_len)\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "total_size = len(dataset)\n",
    "test_size = int(0.1 * total_size)\n",
    "train_size = total_size - test_size\n",
    "train_dataset, _ = random_split(dataset, [train_size, test_size], generator=generator)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = MusicEventRNN(\n",
    "    input_size=len(vocab),\n",
    "    hidden_size=hidden_size,\n",
    "    output_size=len(vocab),\n",
    "    num_layers=num_layers\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "print(f\"Continuing training for {epochs} more epochs with seq_len={seq_len}, hidden_size={hidden_size}...\")\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (x_batch, y_batch) in enumerate(train_loader, start=1):\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = model(x_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 10000 == 0:\n",
    "            avg_loss_so_far = total_loss / batch_idx\n",
    "            print(f\"Epoch {epoch}, Batch {batch_idx}/{len(train_loader)} - Avg Loss So Far: {avg_loss_so_far:.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch}/{epochs} - Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), checkpoint_path)\n",
    "print(f\"Model saved as '{checkpoint_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292b04ad",
   "metadata": {},
   "source": [
    "Generate Music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9aaed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 32\n",
    "hidden_size = 256\n",
    "num_layers = 1\n",
    "\n",
    "dataset = MusicEventDataset(reconstructed_dataset, vocab=vocab, seq_length=seq_len)\n",
    "\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "total_size = len(dataset)\n",
    "test_size = int(0.1 * total_size)\n",
    "train_size = total_size - test_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size], generator=generator)\n",
    "\n",
    "model = MusicEventRNN(\n",
    "    input_size=len(vocab),\n",
    "    hidden_size=hidden_size,\n",
    "    output_size=len(vocab),\n",
    "    num_layers=num_layers\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(\"music_gen_seq32_hidden256.pth\", map_location=device))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f44a0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_music_multihot(model, vocab, start_sequence, max_length=100, temperature=1.0, device='cpu', threshold=0.3):\n",
    "    \"\"\"\n",
    "    Generate music with a model that outputs multi-hot note vectors.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    generated = start_sequence.copy()\n",
    "    input_seq = torch.tensor([vocab.encode_event(ev) for ev in start_sequence], dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    hidden = None\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            output, hidden = model(input_seq, hidden)\n",
    "            logits = output.squeeze(0)\n",
    "            probs = torch.softmax(logits / temperature, dim=0)\n",
    "\n",
    "            bernoulli_samples = torch.bernoulli(probs).int()\n",
    "            sampled = (bernoulli_samples == 1).nonzero(as_tuple=True)[0].tolist()\n",
    "\n",
    "            if not sampled:\n",
    "                topk = probs.topk(3).indices.tolist()\n",
    "                sampled = topk[:random.randint(1, 3)]\n",
    "\n",
    "            decoded_event = [vocab.idx_to_note[idx] for idx in sampled]\n",
    "            if len(decoded_event) == 1:\n",
    "                decoded_event = decoded_event[0]\n",
    "                \n",
    "            generated.append(decoded_event)\n",
    "\n",
    "            input_seq = torch.tensor([[vocab.encode_event(decoded_event)]], dtype=torch.float32, device=device)\n",
    "\n",
    "    return generated[len(start_sequence):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd82bc4",
   "metadata": {},
   "source": [
    "Trial for a single sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "21809e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sequence of notes/chords:\n",
      "[[61, 65], 68, [61, 65], [49, 56], 61, [44, 65], 65, 56, [46, 56], 61, 62, [61, 65], 62, 55, 61, [56, 69], 73, 61, 61, 68, [61, 64, 70], [61, 70, 72], [37, 53], 73, 61, [61, 63, 58], 80, 80, [49, 61, 73, 84], 56, [61, 63], 65, [68, 70, 73], [75, 73], 44, [63, 75, 80], 87, [39, 75, 77], 54, 56, 61, [61, 63], [61, 63], 75, 68, 90, [87, 75], [75, 87, 61], 87, 87]\n"
     ]
    }
   ],
   "source": [
    "start_sequence = reconstructed_dataset['sequence'][1][:32]\n",
    "threshold = 0.7\n",
    "\n",
    "generated_events = generate_music_multihot(model, vocab, start_sequence, max_length=50, temperature=1, device=device, threshold=threshold)\n",
    "\n",
    "print(\"Generated sequence of notes/chords:\")\n",
    "print(generated_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628ab41a",
   "metadata": {},
   "source": [
    "Multiple generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e265991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_midi(events, filename=\"generated_from_test.mid\", default_duration=0.5):\n",
    "    from music21 import stream, note, chord, duration, midi\n",
    "\n",
    "    part = stream.Part()\n",
    "    current_offset = 0.0\n",
    "\n",
    "    for ev in events:\n",
    "        if isinstance(ev, list):\n",
    "            m21_event = chord.Chord(ev)\n",
    "        else:\n",
    "            m21_event = note.Note(ev)\n",
    "\n",
    "        m21_event.duration = duration.Duration(default_duration)\n",
    "        part.insert(current_offset, m21_event)\n",
    "\n",
    "        current_offset += default_duration\n",
    "\n",
    "    score = stream.Score()\n",
    "    score.insert(0, part)\n",
    "\n",
    "    mf = midi.translate.music21ObjectToMidiFile(score)\n",
    "    mf.open(filename, 'wb')\n",
    "    mf.write()\n",
    "    mf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490016e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 32\n",
    "vocab = Vocabulary(reconstructed_dataset)\n",
    "dataset = MusicEventDataset(reconstructed_dataset, vocab, seq_length=seq_len)\n",
    "\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "test_size = int(0.1 * len(dataset))\n",
    "train_size = len(dataset) - test_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size], generator=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "34556c81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MusicEventRNN(\n",
       "  (rnn): LSTM(88, 256, batch_first=True)\n",
       "  (fc): Linear(in_features=256, out_features=88, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MusicEventRNN(input_size=vocab.vocab_size, hidden_size=256, output_size=vocab.vocab_size)\n",
    "model.load_state_dict(torch.load(\"music_gen_seq32_hidden256.pth\", map_location='cpu'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99745eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = \"generated_music_rnn/\"\n",
    "for i in range(100):\n",
    "    x, _ = test_dataset[random.randint(0, len(test_dataset)-1)]\n",
    "    start_seq = [vocab.decode_event(vec) for vec in x]\n",
    "    generated_events = generate_music_multihot(model, vocab, start_seq, max_length=50, temperature=1)\n",
    "\n",
    "    save_to_midi(generated_events, root_folder + f\"dataset/sample_{i}.mid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21759c41",
   "metadata": {},
   "source": [
    "### Generate from popular songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649bfbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_midi_to_event_sequence(midi_path):\n",
    "    \"\"\"\n",
    "    Parses a MIDI file into a list of notes and chords.\n",
    "    Chords become lists of pitch numbers, notes stay as integers.\n",
    "    \"\"\"\n",
    "    score = converter.parse(midi_path)\n",
    "    flat = score.flat.notes\n",
    "    events = []\n",
    "\n",
    "    for el in flat:\n",
    "        if isinstance(el, note.Note):\n",
    "            events.append(el.pitch.midi)\n",
    "        elif isinstance(el, chord.Chord):\n",
    "            events.append([p.midi for p in el.pitches])\n",
    "\n",
    "    return events\n",
    "\n",
    "def prime_and_generate_from_midi(midi_path, model, vocab, max_length=50, temperature=1.0, seq_len=32, output_file=\"output.mid\", def_duration=0.5):\n",
    "    \"\"\"\n",
    "    Loads a MIDI file, extracts a sequence of notes/chords, and uses it to generate new music.\n",
    "    \"\"\"\n",
    "    original_sequence = parse_midi_to_event_sequence(midi_path)\n",
    "\n",
    "    if len(original_sequence) < seq_len:\n",
    "        raise ValueError(f\"Sequence too short: {len(original_sequence)} events (need at least {seq_len})\")\n",
    "\n",
    "    start_sequence = original_sequence[60:60+seq_len]\n",
    "\n",
    "    generated = generate_music_multihot(model, vocab, start_sequence, max_length=max_length, temperature=temperature)\n",
    "\n",
    "    save_to_midi(generated, output_file, default_duration=def_duration)\n",
    "    return output_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "1ae87451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'generated_music_rnn/popular/HipsDontLie.mid'"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from_folder = \"popular_songs/\"\n",
    "name_of_song = \"HipsDontLie.mid\"\n",
    "to_folder = \"generated_music_rnn/popular/\"\n",
    "prime_and_generate_from_midi(from_folder + name_of_song, model, vocab, temperature=0.7, output_file=to_folder + name_of_song, def_duration=0.6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
