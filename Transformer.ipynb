{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ee66e73",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "\n",
    "In this section, we explore the use of a Transformer-based model for the task of jazz piano music generation. We train the model to predict the next musical event — a combination of a pitch and its duration — based on a fixed-length context of previous events. Once trained, the model can be used to generate new musical sequences by iteratively predicting events starting from an initial seed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a59db25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from numpy import e\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "import pretty_midi\n",
    "from music21 import converter, note, chord\n",
    "from tqdm import tqdm\n",
    "\n",
    "root = 'data_processed/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6f2632",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "911625fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_parse_all_columns_df(df):\n",
    "    \"\"\"\n",
    "    Parse all columns in a DataFrame to numeric, coercing errors.\n",
    "    \"\"\"\n",
    "    df['notes'] = df['notes'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    df['chords'] = df['chords'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    df['velocities'] = df['velocities'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    df['durations'] = df['durations'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    df['offsets'] = df['offsets'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    df['ordered_events'] = df['ordered_events'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    return df\n",
    "\n",
    "def load_dataframe_from_two_csvs(file1, file2):\n",
    "    \"\"\"\n",
    "    Load and concatenate two CSV files into a single pandas DataFrame.\n",
    "    \"\"\"\n",
    "    df1 = pd.read_csv(file1)\n",
    "    df2 = pd.read_csv(file2)\n",
    "    full_df = pd.concat([df1, df2], ignore_index=True)\n",
    "    full_df = safe_parse_all_columns_df(full_df)\n",
    "\n",
    "    return full_df\n",
    "\n",
    "def save_dataframe_to_two_csvs(df, file1, file2):\n",
    "    \"\"\"\n",
    "    Split a DataFrame in half and save it into two CSV files.\n",
    "    \"\"\"\n",
    "    halfway = len(df) // 2\n",
    "    df.iloc[:halfway].to_csv(file1, index=False)\n",
    "    df.iloc[halfway:].to_csv(file2, index=False)\n",
    "\n",
    "def load_dataframe_from_one_csv(file):\n",
    "    \"\"\"\n",
    "    Load a DataFrame from a single CSV file.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_dataframe_to_one_csv(df, file):\n",
    "    \"\"\"\n",
    "    Save a DataFrame to a single CSV file.\n",
    "    \"\"\"\n",
    "    df.to_csv(file, index=True)\n",
    "\n",
    "def load_reconstructed_events(file):\n",
    "    \"\"\"\n",
    "    Loads the reconstructed events CSV and safely parses the 'sequence' column,\n",
    "    converting notes to integers and chords to lists of integers.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    def safe_parse(seq_str):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(seq_str)\n",
    "            if not isinstance(parsed, list):\n",
    "                raise ValueError(\"Parsed sequence is not a list\")\n",
    "\n",
    "            normalized = []\n",
    "            for el in parsed:\n",
    "                if isinstance(el, list):\n",
    "                    normalized.append([int(x) for x in el])\n",
    "                else:\n",
    "                    normalized.append(int(el))\n",
    "            return normalized\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing sequence: {seq_str}\")\n",
    "            raise e\n",
    "\n",
    "    df['sequence'] = df['sequence'].apply(safe_parse)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43a5ae81",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = root + 'data_part1.csv'\n",
    "file2 = root + 'data_part2.csv'\n",
    "\n",
    "df = load_dataframe_from_two_csvs(file1, file2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe21f25",
   "metadata": {},
   "source": [
    "### Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30071ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_chord_to_list(chord):\n",
    "    \"\"\"\n",
    "    Convert a chord string to a list of integers.\n",
    "    \"\"\"\n",
    "    if isinstance(chord, str):\n",
    "        print([int(x) for x in chord.split(',') if x.isdigit()])\n",
    "        return [int(x) for x in chord.split(',') if x.isdigit()]\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c80d128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_ordered_events(df):\n",
    "    \"\"\"\n",
    "    Reconstruct the ordered list of events (notes and chords) for each song.\n",
    "    \"\"\"\n",
    "    sequences  = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        idx_note = 0\n",
    "        idx_chord = 0\n",
    "        reconstructed = []\n",
    "\n",
    "        for element in df['ordered_events'][i]:\n",
    "            if element == 'n':\n",
    "                reconstructed.append(df['notes'][i][idx_note])\n",
    "                idx_note += 1\n",
    "            elif element == 'c':\n",
    "                parsed_chord = parse_chord_to_list(df['chords'][i][idx_chord])\n",
    "                reconstructed.append(df['chords'][i][idx_chord])\n",
    "                idx_chord += 1\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown event type: {e}\")\n",
    "        \n",
    "        sequences.append(reconstructed)\n",
    "\n",
    "    reconstructed_dataset = pd.DataFrame({'sequence': sequences})\n",
    "    reconstructed_dataset.index.name = 'index'\n",
    "\n",
    "    return reconstructed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c99b4f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataframe_to_one_csv(reconstruct_ordered_events(df), root + 'reconstructed_ordered_events.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1c3b1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_dataset = load_reconstructed_events(root + 'reconstructed_ordered_events.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8985a9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_events_with_durations = reconstructed_dataset.copy()\n",
    "ordered_events_with_durations['durations'] = df['durations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13058aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sequence</th>\n",
       "      <th>durations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[88, 38, 38, 45, 45, 45, [44, 45], 45, 45, 38,...</td>\n",
       "      <td>[0.25, 2.5, 0.25, 0.3333, 0.3333, 0.25, 0.3333...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[[56, 65, 68, 80, 60], [54, 61, 65, 75], 77, 7...</td>\n",
       "      <td>[1.25, 1.6667, 0.25, 1.0, 1.0, 0.5, 0.6667, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[46, 70, [53, 60, 62], [65, 46, 53, 60], 70, [...</td>\n",
       "      <td>[2.6667, 0.25, 1.25, 0.75, 0.75, 1.6667, 1.75,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[[52, 28, 40], 64, [65, 68], [53, 55, 56], [58...</td>\n",
       "      <td>[3.75, 2.5, 1.0, 2.0, 1.6667, 1.3333, 0.75, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[59, 59, [60, 62, 38, 54], 62, [43, 62, 50, 52...</td>\n",
       "      <td>[0.75, 0.6667, 0.6667, 0.5, 1.0, 0.25, 2.0, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2770</th>\n",
       "      <td>2770</td>\n",
       "      <td>[61, 49, [63, 60], 65, 70, [58, 61, 66, 49], [...</td>\n",
       "      <td>[1.3333, 1.6667, 1.0, 0.5, 0.5, 0.5, 0.6667, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2771</th>\n",
       "      <td>2771</td>\n",
       "      <td>[[45, 55, 67, 69, 72, 75], [46, 74, 56, 62, 68...</td>\n",
       "      <td>[1.0, 1.0, 0.25, 1.25, 0.3333, 0.25, 0.25, 0.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2772</th>\n",
       "      <td>2772</td>\n",
       "      <td>[[45, 57], 71, 52, 61, 64, 45, 69, [45, 69], 4...</td>\n",
       "      <td>[1.6667, 0.6667, 3.3333, 2.75, 2.25, 0.75, 0.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2773</th>\n",
       "      <td>2773</td>\n",
       "      <td>[48, 41, 60, [67, 68, 72, 79], 66, [65, 68, 72...</td>\n",
       "      <td>[0.3333, 0.3333, 0.25, 0.25, 0.3333, 0.3333, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2774</th>\n",
       "      <td>2774</td>\n",
       "      <td>[63, 57, 60, 59, 58, 56, 56, 63, 62, 55, 57, 6...</td>\n",
       "      <td>[2.0, 0.5, 0.25, 0.5, 0.3333, 2.3333, 0.5, 0.6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2775 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                           sequence  \\\n",
       "0         0  [88, 38, 38, 45, 45, 45, [44, 45], 45, 45, 38,...   \n",
       "1         1  [[56, 65, 68, 80, 60], [54, 61, 65, 75], 77, 7...   \n",
       "2         2  [46, 70, [53, 60, 62], [65, 46, 53, 60], 70, [...   \n",
       "3         3  [[52, 28, 40], 64, [65, 68], [53, 55, 56], [58...   \n",
       "4         4  [59, 59, [60, 62, 38, 54], 62, [43, 62, 50, 52...   \n",
       "...     ...                                                ...   \n",
       "2770   2770  [61, 49, [63, 60], 65, 70, [58, 61, 66, 49], [...   \n",
       "2771   2771  [[45, 55, 67, 69, 72, 75], [46, 74, 56, 62, 68...   \n",
       "2772   2772  [[45, 57], 71, 52, 61, 64, 45, 69, [45, 69], 4...   \n",
       "2773   2773  [48, 41, 60, [67, 68, 72, 79], 66, [65, 68, 72...   \n",
       "2774   2774  [63, 57, 60, 59, 58, 56, 56, 63, 62, 55, 57, 6...   \n",
       "\n",
       "                                              durations  \n",
       "0     [0.25, 2.5, 0.25, 0.3333, 0.3333, 0.25, 0.3333...  \n",
       "1     [1.25, 1.6667, 0.25, 1.0, 1.0, 0.5, 0.6667, 0....  \n",
       "2     [2.6667, 0.25, 1.25, 0.75, 0.75, 1.6667, 1.75,...  \n",
       "3     [3.75, 2.5, 1.0, 2.0, 1.6667, 1.3333, 0.75, 0....  \n",
       "4     [0.75, 0.6667, 0.6667, 0.5, 1.0, 0.25, 2.0, 0....  \n",
       "...                                                 ...  \n",
       "2770  [1.3333, 1.6667, 1.0, 0.5, 0.5, 0.5, 0.6667, 0...  \n",
       "2771  [1.0, 1.0, 0.25, 1.25, 0.3333, 0.25, 0.25, 0.7...  \n",
       "2772  [1.6667, 0.6667, 3.3333, 2.75, 2.25, 0.75, 0.5...  \n",
       "2773  [0.3333, 0.3333, 0.25, 0.25, 0.3333, 0.3333, 0...  \n",
       "2774  [2.0, 0.5, 0.25, 0.5, 0.3333, 2.3333, 0.5, 0.6...  \n",
       "\n",
       "[2775 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_events_with_durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c05a0d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All rows have matching 'sequence' and 'durations' lengths.\n"
     ]
    }
   ],
   "source": [
    "mismatches = ordered_events_with_durations.apply(\n",
    "    lambda row: len(row['sequence']) != len(row['durations']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "invalid_rows = ordered_events_with_durations[mismatches]\n",
    "\n",
    "\n",
    "if not invalid_rows.empty:\n",
    "    print(\"Rows with mismatched 'sequence' and 'durations':\")\n",
    "    print(invalid_rows)\n",
    "else:\n",
    "    print(\"All rows have matching 'sequence' and 'durations' lengths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a883e280",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataframe_to_one_csv((ordered_events_with_durations), root + 'ordered_events_with_durations.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441a74df",
   "metadata": {},
   "source": [
    "## Tokenization Strategy\n",
    "\n",
    "Before training, each music track is represented in a structured format as a row in a DataFrame. Each row contains two key sequences:\n",
    "- `sequence`: a list of events, where each event is either a single note (as an integer) or a chord (as a list of integers).\n",
    "- `durations`: a list of float values representing how long each event is held.\n",
    "\n",
    "To convert these symbolic sequences into a format suitable for Transformer training, we use a custom `TokenVocabulary` class. This class encodes each pair of `(event, duration)` into two tokens:\n",
    "- A **note/chord token**: formatted as `NOTE_<pitch>` for single notes (e.g., `NOTE_60`) or `CHORD_<pitch1>_<pitch2>_...` for chords (e.g., `CHORD_60_64_67`).\n",
    "- A **duration token**: formatted as `DUR_<duration>` (e.g., `DUR_0.500`).\n",
    "\n",
    "The `encode_sequence_and_durations` method processes each song by alternating between these two types of tokens, producing sequences like:\n",
    "[NOTE_60, DUR_0.500, NOTE_62, DUR_0.250, CHORD_60_64_67, DUR_1.000, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d766d68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenVocabulary:\n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        Build a vocabulary of all unique tokens (note, chord, duration).\n",
    "        \"\"\"\n",
    "        token_set = set()\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            for event, dur in zip(row['sequence'], row['durations']):\n",
    "                if isinstance(event, list):\n",
    "                    note_token = 'CHORD_' + '_'.join(map(str, sorted(event)))\n",
    "                else:\n",
    "                    note_token = f'NOTE_{event}'\n",
    "                dur_token = f'DUR_{dur:.3f}'\n",
    "                token_set.update([note_token, dur_token])\n",
    "\n",
    "        self.tokens = sorted(token_set)\n",
    "        self.token_to_idx = {tok: idx for idx, tok in enumerate(self.tokens)}\n",
    "        self.idx_to_token = {idx: tok for tok, idx in self.token_to_idx.items()}\n",
    "        self.vocab_size = len(self.tokens)\n",
    "\n",
    "    def encode(self, token):\n",
    "        return self.token_to_idx[token]\n",
    "\n",
    "    def decode(self, index):\n",
    "        return self.idx_to_token[index]\n",
    "\n",
    "    def encode_sequence_and_durations(self, sequence, durations):\n",
    "        \"\"\"\n",
    "        Given a sequence of events and durations, returns a list of token IDs\n",
    "        in alternating [note_token, dur_token, note_token, dur_token, ...] format.\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        for event, dur in zip(sequence, durations):\n",
    "            if isinstance(event, list):\n",
    "                note_token = 'CHORD_' + '_'.join(map(str, sorted(event)))\n",
    "            else:\n",
    "                note_token = f'NOTE_{event}'\n",
    "            dur_token = f'DUR_{dur:.3f}'\n",
    "            tokens.extend([note_token, dur_token])\n",
    "        return [self.encode(token) for token in tokens]\n",
    "    \n",
    "    def decode_sequence_and_durations(self, token_ids):\n",
    "        \"\"\"\n",
    "        Converts a flat list of token IDs (note/dur alternating) into\n",
    "        a list of (event, duration) pairs.\n",
    "\n",
    "        Returns:\n",
    "            List of tuples: (event, duration), where event is int or list[int], duration is float\n",
    "        \"\"\"\n",
    "        decoded = [self.decode(tid) for tid in token_ids]\n",
    "        assert len(decoded) % 2 == 0, \"Token sequence should be even-length (note-dur pairs).\"\n",
    "\n",
    "        event_sequence = []\n",
    "        for i in range(0, len(decoded), 2):\n",
    "            note_token = decoded[i]\n",
    "            dur_token = decoded[i + 1]\n",
    "\n",
    "            # Parse note/chord\n",
    "            if note_token.startswith('NOTE_'):\n",
    "                event = int(note_token.replace('NOTE_', ''))\n",
    "            elif note_token.startswith('CHORD_'):\n",
    "                event = list(map(int, note_token.replace('CHORD_', '').split('_')))\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid note token: {note_token}\")\n",
    "\n",
    "            # Parse duration\n",
    "            if dur_token.startswith('DUR_'):\n",
    "                duration = float(dur_token.replace('DUR_', ''))\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid duration token: {dur_token}\")\n",
    "\n",
    "            event_sequence.append((event, duration))\n",
    "\n",
    "        return event_sequence\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8694c41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 154107\n"
     ]
    }
   ],
   "source": [
    "vocab = TokenVocabulary(ordered_events_with_durations)\n",
    "print(\"Total tokens:\", vocab.vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d6f8397",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizedMusicDataset(Dataset):\n",
    "    def __init__(self, df, vocab, context_length=20):\n",
    "        self.samples = []\n",
    "        self.vocab = vocab\n",
    "        self.context_length = context_length\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            sequence = self.vocab.encode_sequence_and_durations(row['sequence'], row['durations'])\n",
    "            n_tokens = len(sequence)\n",
    "\n",
    "            # Since each event is 2 tokens, step by 2\n",
    "            if n_tokens <= 2 * context_length + 2:\n",
    "                continue\n",
    "\n",
    "            for i in range(0, n_tokens - 2 * context_length - 2 + 1, 2):\n",
    "                context = sequence[i : i + 2 * context_length]  # 2 tokens per event\n",
    "                target = sequence[i + 2 * context_length : i + 2 * context_length + 2]  # next event (2 tokens)\n",
    "                self.samples.append((context, target))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context, target = self.samples[idx]\n",
    "        context_tensor = torch.tensor(context, dtype=torch.long)\n",
    "        target_tensor = torch.tensor(target, dtype=torch.long)\n",
    "        return context_tensor, target_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e58ddae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TokenizedMusicDataset(ordered_events_with_durations, vocab, context_length=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fffe633e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token IDs: tensor([154095, 153461, 154045, 153608, 154045, 153461, 154052, 153462, 154052,\n",
      "        153462, 154052, 153461,  51829, 153462, 154052, 153461, 154052, 153462,\n",
      "        154045, 153467, 112860, 153476,  51829, 153462, 154052, 153461, 154068,\n",
      "        153470, 154053, 153462, 154053, 153462,  23927, 153479, 154064, 153461,\n",
      "        154045, 153475, 154053, 153462])\n",
      "Target token ID: tensor([154045, 153470])\n",
      "Decoded input tokens: ['NOTE_88', 'DUR_0.250', 'NOTE_38', 'DUR_2.500', 'NOTE_38', 'DUR_0.250', 'NOTE_45', 'DUR_0.333', 'NOTE_45', 'DUR_0.333', 'NOTE_45', 'DUR_0.250', 'CHORD_44_45', 'DUR_0.333', 'NOTE_45', 'DUR_0.250', 'NOTE_45', 'DUR_0.333', 'NOTE_38', 'DUR_0.750', 'CHORD_55_60', 'DUR_1.500', 'CHORD_44_45', 'DUR_0.333', 'NOTE_45', 'DUR_0.250', 'NOTE_61', 'DUR_1.000', 'NOTE_46', 'DUR_0.333', 'NOTE_46', 'DUR_0.333', 'CHORD_38_65', 'DUR_1.750', 'NOTE_57', 'DUR_0.250', 'NOTE_38', 'DUR_1.417', 'NOTE_46', 'DUR_0.333']\n",
      "Decoded target token: ['NOTE_38', 'DUR_1.000']\n"
     ]
    }
   ],
   "source": [
    "x, y = dataset[0]\n",
    "print(\"Input token IDs:\", x)\n",
    "print(\"Target token ID:\", y)\n",
    "print(\"Decoded input tokens:\", [vocab.decode(i.item()) for i in x])\n",
    "print(\"Decoded target token:\", [vocab.decode(i.item()) for i in y])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402355da",
   "metadata": {},
   "source": [
    "### Efficient Training via Sampling\n",
    "\n",
    "Due to the large size of the dataset — and the high computational cost of training a Transformer model on all available sequences — we adopted a more efficient sampling-based strategy for training.\n",
    "\n",
    "Instead of feeding entire tokenized tracks into the model, we use the `SampledMusicDataset` class to extract a fixed number of random training samples from each song. For every track in the dataset:\n",
    "- The entire event-duration sequence is first tokenized as described earlier.\n",
    "- Then, instead of using every possible subsequence, we randomly select a small number of **valid start points** within the sequence.\n",
    "- From each chosen start point, we extract:\n",
    "  - A **context** of `context_length` events (i.e., `2 * context_length` tokens).\n",
    "  - The **target** event — the next pair of tokens immediately following the context.\n",
    "\n",
    "By training on these randomized subsequences, the model learns to generalize from diverse musical contexts without the overhead of processing full-length tracks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee7d29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampledMusicDataset(Dataset):\n",
    "    def __init__(self, df, vocab, context_length=20, samples_per_song=5):\n",
    "        self.samples = []\n",
    "        self.vocab = vocab\n",
    "        self.context_length = context_length\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            sequence = self.vocab.encode_sequence_and_durations(row['sequence'], row['durations'])\n",
    "            n_tokens = len(sequence)\n",
    "\n",
    "            max_start = n_tokens - 2 * context_length - 2\n",
    "            if max_start < 0:\n",
    "                continue  # skip very short songs\n",
    "\n",
    "            # Collect all valid even start indices\n",
    "            valid_starts = [i for i in range(0, max_start + 1, 2)]\n",
    "            if not valid_starts:\n",
    "                continue\n",
    "\n",
    "            # Sample from valid starts\n",
    "            for _ in range(samples_per_song):\n",
    "                start = random.choice(valid_starts)\n",
    "                context = sequence[start : start + 2 * context_length]\n",
    "                target = sequence[start + 2 * context_length : start + 2 * context_length + 2]\n",
    "                self.samples.append((context, target))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context, target = self.samples[idx]\n",
    "        context_tensor = torch.tensor(context, dtype=torch.long)\n",
    "        target_tensor = torch.tensor(target, dtype=torch.long)\n",
    "        return context_tensor, target_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75353a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data = SampledMusicDataset(ordered_events_with_durations, vocab, context_length=20, samples_per_song=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "387af849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token IDs: tensor([ 95523, 153467,  17453, 153461, 154059, 153740,  77853, 153461, 154081,\n",
      "        153740, 154055, 153470, 154055, 153611, 150266, 153478, 154098, 153476,\n",
      "        154088, 153734, 154065, 153462, 154071, 153610, 154089, 153479, 154084,\n",
      "        153466, 154067, 153602, 129955, 153464, 154065, 153467, 154065, 153470,\n",
      "        144481, 153466, 146040, 153605])\n",
      "Target token ID: tensor([152082, 153474])\n",
      "Decoded input tokens: ['CHORD_52_58_62', 'DUR_0.750', 'CHORD_36_70', 'DUR_0.250', 'NOTE_52', 'DUR_3.500', 'CHORD_48_58_65_70', 'DUR_0.250', 'NOTE_74', 'DUR_3.500', 'NOTE_48', 'DUR_1.000', 'NOTE_48', 'DUR_2.750', 'CHORD_79_86', 'DUR_1.667', 'NOTE_91', 'DUR_1.500', 'NOTE_81', 'DUR_3.000', 'NOTE_58', 'DUR_0.333', 'NOTE_64', 'DUR_2.667', 'NOTE_82', 'DUR_1.750', 'NOTE_77', 'DUR_0.667', 'NOTE_60', 'DUR_2.000', 'CHORD_58_70', 'DUR_0.500', 'NOTE_58', 'DUR_0.750', 'NOTE_58', 'DUR_1.000', 'CHORD_67_74', 'DUR_0.667', 'CHORD_69_93_98', 'DUR_2.250']\n",
      "Decoded target token: ['CHORD_89_94', 'DUR_1.333']\n"
     ]
    }
   ],
   "source": [
    "x, y = sampled_data[0]\n",
    "print(\"Input token IDs:\", x)\n",
    "print(\"Target token ID:\", y)\n",
    "print(\"Decoded input tokens:\", [vocab.decode(i.item()) for i in x])\n",
    "print(\"Decoded target token:\", [vocab.decode(i.item()) for i in y])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6101584",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7fa983",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=64, nhead=2, num_layers=2, dropout=0.1, max_seq_len=512):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [batch_size, seq_len] of token indices\n",
    "        \"\"\"\n",
    "        B, T = x.size()\n",
    "        token_emb = self.token_embedding(x)                         # [B, T, d_model]\n",
    "        pos_ids = torch.arange(T, device=x.device).unsqueeze(0)     # [1, T]\n",
    "        pos_emb = self.pos_embedding(pos_ids)                       # [1, T, d_model]\n",
    "\n",
    "        x = token_emb + pos_emb                                     # [B, T, d_model]\n",
    "        x = self.transformer(x)                                     # [B, T, d_model]\n",
    "        logits = self.output_layer(x)                               # [B, T, vocab_size]\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96948e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = vocab.vocab_size\n",
    "model = MusicTransformer(vocab_size=vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbaf986",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device( \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f1bbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(sampled_data, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e340b1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 868/868 [19:47<00:00,  1.37s/it, loss=11.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 complete | Avg Loss: 9.4007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 868/868 [18:02<00:00,  1.25s/it, loss=7.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 complete | Avg Loss: 8.9449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 868/868 [18:29<00:00,  1.28s/it, loss=8.18]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 complete | Avg Loss: 8.6550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}\")\n",
    "\n",
    "    for batch_idx, (x, y) in progress_bar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        note_logits = logits[:, -2, :]\n",
    "        dur_logits = logits[:, -1, :]\n",
    "\n",
    "        loss_note = loss_fn(note_logits, y[:, 0])\n",
    "        loss_dur = loss_fn(dur_logits, y[:, 1])\n",
    "        loss = loss_note + loss_dur\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1} complete | Avg Loss: {avg_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0821925d",
   "metadata": {},
   "source": [
    "### Generating music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a819fdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_music(model, vocab, seed_sequence, seed_durations, context_length, generate_events, temperature, device):\n",
    "    \"\"\"\n",
    "    Generate music by alternating between note/chord and duration tokens.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    tokens = vocab.encode_sequence_and_durations(seed_sequence, seed_durations)\n",
    "    \n",
    "\n",
    "\n",
    "    generated_tokens = tokens.copy()\n",
    "\n",
    "    # Decide what type of token comes next\n",
    "    next_is_note = (len(generated_tokens) % 2 == 0)  # even index → note, odd → dur\n",
    "\n",
    "    for _ in range(generate_events * 2):\n",
    "        context = generated_tokens[-context_length:] if len(generated_tokens) > context_length else generated_tokens\n",
    "        input_tensor = torch.tensor(context, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_tensor)\n",
    "\n",
    "        next_token_logits = logits[0, -1, :] / temperature\n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "\n",
    "        # Mask out invalid tokens (either note or duration)\n",
    "        filtered_probs = probs.clone()\n",
    "        for i, token in enumerate(vocab.tokens):\n",
    "            if next_is_note and token.startswith(\"DUR_\"):\n",
    "                filtered_probs[i] = 0\n",
    "            elif not next_is_note and (token.startswith(\"NOTE_\") or token.startswith(\"CHORD_\")):\n",
    "                filtered_probs[i] = 0\n",
    "\n",
    "        if filtered_probs.sum() == 0:\n",
    "            raise ValueError(\"All probabilities filtered out — check vocab consistency or model behavior.\")\n",
    "\n",
    "        filtered_probs = filtered_probs / filtered_probs.sum()\n",
    "        next_token = torch.multinomial(filtered_probs, num_samples=1).item()\n",
    "        generated_tokens.append(next_token)\n",
    "        next_is_note = not next_is_note  # alternate\n",
    "\n",
    "    return vocab.decode_sequence_and_durations(generated_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5030d953",
   "metadata": {},
   "source": [
    "#### Trial on a single sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4eda8fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sequence:\n",
      "1: 88 (0.25)\n",
      "2: 38 (2.5)\n",
      "3: 38 (0.25)\n",
      "4: 45 (0.333)\n",
      "5: 45 (0.333)\n",
      "6: 45 (0.25)\n",
      "7: [44, 45] (0.333)\n",
      "8: 45 (0.25)\n",
      "9: 45 (0.333)\n",
      "10: 38 (0.75)\n",
      "11: [55, 60] (1.5)\n",
      "12: [44, 45] (0.333)\n",
      "13: 45 (0.25)\n",
      "14: 61 (1.0)\n",
      "15: 46 (0.333)\n",
      "16: 46 (0.333)\n",
      "17: [38, 65] (1.75)\n",
      "18: 57 (0.25)\n",
      "19: 38 (1.417)\n",
      "20: 46 (0.333)\n",
      "21: 84 (0.333)\n",
      "22: 48 (0.25)\n",
      "23: 48 (0.5)\n",
      "24: 58 (0.333)\n",
      "25: 56 (0.25)\n",
      "26: 52 (0.5)\n",
      "27: [46, 56, 62, 70] (0.333)\n",
      "28: 72 (0.167)\n",
      "29: 50 (0.5)\n",
      "30: 86 (0.25)\n",
      "31: 37 (1.75)\n",
      "32: [36, 48] (1.25)\n",
      "33: 77 (0.25)\n",
      "34: 58 (0.25)\n",
      "35: 66 (0.333)\n",
      "36: 93 (1.25)\n",
      "37: 77 (0.25)\n",
      "38: 85 (0.25)\n",
      "39: [56, 60, 65] (2.333)\n",
      "40: 88 (0.833)\n",
      "41: 95 (0.5)\n",
      "42: 69 (1.75)\n",
      "43: 83 (1.25)\n",
      "44: 97 (0.25)\n",
      "45: 79 (2.0)\n",
      "46: 81 (3.667)\n",
      "47: 80 (0.75)\n",
      "48: 35 (0.667)\n",
      "49: [54, 64] (2.0)\n",
      "50: 76 (1.5)\n",
      "51: 82 (0.667)\n",
      "52: 31 (0.25)\n",
      "53: 52 (1.333)\n",
      "54: 46 (0.25)\n",
      "55: 51 (0.5)\n",
      "56: [54, 63] (0.25)\n",
      "57: 49 (0.5)\n",
      "58: 52 (0.667)\n",
      "59: 86 (0.5)\n",
      "60: 50 (0.25)\n",
      "61: 86 (0.25)\n",
      "62: 101 (0.5)\n",
      "63: 83 (0.25)\n",
      "64: 74 (0.25)\n",
      "65: 81 (0.75)\n",
      "66: 58 (0.667)\n",
      "67: 70 (2.333)\n",
      "68: 45 (0.25)\n",
      "69: 60 (0.25)\n",
      "70: [55, 60] (0.25)\n"
     ]
    }
   ],
   "source": [
    "# Select the first track from the dataset\n",
    "track_index = 0\n",
    "seed_seq = ordered_events_with_durations.iloc[track_index]['sequence']\n",
    "seed_durs = ordered_events_with_durations.iloc[track_index]['durations']\n",
    "\n",
    "# Generation settings\n",
    "context_length = 20\n",
    "generate_events = 50\n",
    "temperature = 0.8\n",
    "\n",
    "# Generate music\n",
    "generated_sequence = generate_music(\n",
    "    model=model,\n",
    "    vocab=vocab,\n",
    "    seed_sequence=seed_seq[:context_length],\n",
    "    seed_durations=seed_durs[:context_length],\n",
    "    context_length=context_length,\n",
    "    generate_events=generate_events,\n",
    "    temperature=temperature,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(\"Generated sequence:\")\n",
    "for i, (event, dur) in enumerate(generated_sequence):\n",
    "    print(f\"{i+1}: {event} ({dur})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51785e2c",
   "metadata": {},
   "source": [
    "#### Multiple generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfc5a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_to_midi(event_sequence, output_path=\"generated.mid\", program=0):\n",
    "    \"\"\"\n",
    "    Converts a sequence of (note or chord, duration) into a MIDI file.\n",
    "\n",
    "    Args:\n",
    "        event_sequence: List of (note:int or chord:list[int], duration:float)\n",
    "        output_path: Path to save the output MIDI file\n",
    "        program: MIDI instrument program number (0 = Acoustic Grand Piano)\n",
    "\n",
    "    Returns:\n",
    "        None (writes MIDI file to disk)\n",
    "    \"\"\"\n",
    "    midi = pretty_midi.PrettyMIDI()\n",
    "    instrument = pretty_midi.Instrument(program=program)\n",
    "\n",
    "    current_time = 0.0\n",
    "    for event, duration in event_sequence:\n",
    "        if isinstance(event, int):\n",
    "            note = pretty_midi.Note(\n",
    "                velocity=100,\n",
    "                pitch=event,\n",
    "                start=current_time,\n",
    "                end=current_time + duration\n",
    "            )\n",
    "            instrument.notes.append(note)\n",
    "        elif isinstance(event, list):\n",
    "            for pitch in event:\n",
    "                note = pretty_midi.Note(\n",
    "                    velocity=100,\n",
    "                    pitch=pitch,\n",
    "                    start=current_time,\n",
    "                    end=current_time + duration\n",
    "                )\n",
    "                instrument.notes.append(note)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid event type: {event}\")\n",
    "        current_time += duration\n",
    "\n",
    "    midi.instruments.append(instrument)\n",
    "    midi.write(output_path)\n",
    "    print(f\"MIDI file written to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5751aba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIDI file written to: generated_music_transformer/dataset\\sample_0.mid\n",
      "[1/10] Saved: generated_music_transformer/dataset\\sample_0.mid\n",
      "MIDI file written to: generated_music_transformer/dataset\\sample_1.mid\n",
      "[2/10] Saved: generated_music_transformer/dataset\\sample_1.mid\n",
      "MIDI file written to: generated_music_transformer/dataset\\sample_2.mid\n",
      "[3/10] Saved: generated_music_transformer/dataset\\sample_2.mid\n",
      "MIDI file written to: generated_music_transformer/dataset\\sample_3.mid\n",
      "[4/10] Saved: generated_music_transformer/dataset\\sample_3.mid\n",
      "MIDI file written to: generated_music_transformer/dataset\\sample_4.mid\n",
      "[5/10] Saved: generated_music_transformer/dataset\\sample_4.mid\n",
      "MIDI file written to: generated_music_transformer/dataset\\sample_5.mid\n",
      "[6/10] Saved: generated_music_transformer/dataset\\sample_5.mid\n",
      "MIDI file written to: generated_music_transformer/dataset\\sample_6.mid\n",
      "[7/10] Saved: generated_music_transformer/dataset\\sample_6.mid\n",
      "MIDI file written to: generated_music_transformer/dataset\\sample_7.mid\n",
      "[8/10] Saved: generated_music_transformer/dataset\\sample_7.mid\n",
      "MIDI file written to: generated_music_transformer/dataset\\sample_8.mid\n",
      "[9/10] Saved: generated_music_transformer/dataset\\sample_8.mid\n",
      "MIDI file written to: generated_music_transformer/dataset\\sample_9.mid\n",
      "[10/10] Saved: generated_music_transformer/dataset\\sample_9.mid\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output_dir = \"generated_music_transformer/dataset\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "context_length = 20     \n",
    "generate_events = 50    \n",
    "temperature = 0.9       \n",
    "\n",
    "for i in range(10):\n",
    "    row = ordered_events_with_durations.iloc[i]\n",
    "    seed_seq = row['sequence'][:context_length]\n",
    "    seed_durs = row['durations'][:context_length]\n",
    "\n",
    "    # Ensure matching length\n",
    "    if len(seed_seq) != len(seed_durs) or len(seed_seq) < context_length:\n",
    "        print(f\"Skipping track {i} due to length mismatch or insufficient seed.\")\n",
    "        continue\n",
    "\n",
    "    # Generate music\n",
    "    generated_sequence = generate_music(\n",
    "        model=model,\n",
    "        vocab=vocab,\n",
    "        seed_sequence=seed_seq,\n",
    "        seed_durations=seed_durs,\n",
    "        context_length=context_length,\n",
    "        generate_events=generate_events,\n",
    "        temperature=temperature,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Save as MIDI\n",
    "    output_path = os.path.join(output_dir, f\"sample_{i}.mid\")\n",
    "    sequence_to_midi(generated_sequence, output_path=output_path)\n",
    "\n",
    "    print(f\"[{i+1}/100] Saved: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdf8461",
   "metadata": {},
   "source": [
    "#### Generation from popular songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7c66eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def midi_to_event_duration_sequence(midi_path):\n",
    "    \"\"\"\n",
    "    Convert a MIDI file to a (sequence, durations) tuple.\n",
    "    sequence: list of ints or list of lists (for chords)\n",
    "    durations: list of floats\n",
    "    \"\"\"\n",
    "    try:\n",
    "        score = converter.parse(midi_path)\n",
    "        flat_notes = score.flat.notes\n",
    "\n",
    "        sequence = []\n",
    "        durations = []\n",
    "\n",
    "        for element in flat_notes:\n",
    "            if isinstance(element, note.Note):\n",
    "                sequence.append(element.pitch.midi)\n",
    "                durations.append(float(element.quarterLength))\n",
    "            elif isinstance(element, chord.Chord):\n",
    "                sequence.append(sorted(p.midi for p in element.pitches))\n",
    "                durations.append(float(element.quarterLength))\n",
    "\n",
    "        return sequence, durations\n",
    "    except Exception as e:\n",
    "        return f\"Error processing {midi_path}: {str(e)}\", [], []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "99fd9d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\claud\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\music21\\stream\\base.py:3675: Music21DeprecationWarning: .flat is deprecated.  Call .flatten() instead\n",
      "  return self.iter().getElementsByClass(classFilterList)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['AnotherBrickInTheWall.mid', 'BackInBlack.mid', 'BillieJean.mid', 'DancingQueen.mid', 'FinalCountdown.mid', 'Hallelujah.mid', 'HappyBirthday.mid', 'HipsDontLie.mid', 'HotelCalifornia.mid', 'PokerFace.mid', 'Titanic.mid', 'Umbrella.mid'])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "midi_folder = \"popular_songs\"\n",
    "converted_data = {}\n",
    "\n",
    "for filename in os.listdir(midi_folder):\n",
    "    if filename.endswith(\".mid\") or filename.endswith(\".midi\"):\n",
    "        midi_path = os.path.join(midi_folder, filename)\n",
    "        result = midi_to_event_duration_sequence(midi_path)\n",
    "        if isinstance(result, tuple):\n",
    "            sequence, durations = result\n",
    "            converted_data[filename] = {\n",
    "                \"sequence\": sequence,\n",
    "                \"durations\": durations\n",
    "            }\n",
    "        else:\n",
    "            print(result[0])  # Print error message if any\n",
    "\n",
    "converted_data.keys()  # Show which files were processed successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2ec2949d",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_points = {'AnotherBrickInTheWall.mid': 61, 'BackInBlack.mid': 60, 'BillieJean.mid': 50, 'DancingQueen.mid': 80, 'FinalCountdown.mid': 10, 'Hallelujah.mid':80, 'HappyBirthday.mid': 40, 'HipsDontLie.mid': 60, 'HotelCalifornia.mid': 50, 'PokerFace.mid': 280, 'Titanic.mid': 60, 'Umbrella.mid': 60}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9725ee",
   "metadata": {},
   "source": [
    "The vocabulary was built only from the training dataset and may not include all possible chords from the popular songs (converted_data).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013bd591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_popular_songs_in_vocab(vocab, converted_data, starting_points, context_length):\n",
    "    \"\"\"\n",
    "    Check each popular song to see if the slice from start_idx with length context_length\n",
    "    contains any tokens not in the vocab.\n",
    "    Prints songs with unknown tokens.\n",
    "    Returns a list of filenames that are safe to use (all tokens known in that slice).\n",
    "    \"\"\"\n",
    "    safe_songs = []\n",
    "\n",
    "    for filename, start_idx in starting_points.items():\n",
    "        if filename not in converted_data:\n",
    "            print(f\"Skipping {filename} (not in converted_data)\")\n",
    "            continue\n",
    "\n",
    "        song_data = converted_data[filename]\n",
    "        sequence = song_data['sequence']\n",
    "        durations = song_data['durations']\n",
    "\n",
    "        \n",
    "        if len(sequence) < start_idx + context_length or len(durations) < start_idx + context_length:\n",
    "            print(f\"Skipping {filename} (not enough events after start index)\")\n",
    "            continue\n",
    "\n",
    "        seq_slice = sequence[start_idx : start_idx + context_length]\n",
    "        dur_slice = durations[start_idx : start_idx + context_length]\n",
    "\n",
    "        unknown_tokens_found = False\n",
    "\n",
    "        for event, dur in zip(seq_slice, dur_slice):\n",
    "            if isinstance(event, list):\n",
    "                note_token = 'CHORD_' + '_'.join(map(str, sorted(event)))\n",
    "            else:\n",
    "                note_token = f'NOTE_{event}'\n",
    "            dur_token = f'DUR_{dur:.3f}'\n",
    "\n",
    "            if note_token not in vocab.token_to_idx or dur_token not in vocab.token_to_idx:\n",
    "                unknown_tokens_found = True\n",
    "                break\n",
    "\n",
    "        if unknown_tokens_found:\n",
    "            print(f\"Unknown tokens in {filename} — skipping this song.\")\n",
    "        else:\n",
    "            safe_songs.append(filename)\n",
    "\n",
    "    return safe_songs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f845a0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown tokens in DancingQueen.mid — skipping this song.\n",
      "Unknown tokens in HipsDontLie.mid — skipping this song.\n",
      "Unknown tokens in PokerFace.mid — skipping this song.\n"
     ]
    }
   ],
   "source": [
    "safe_popular_songs = check_popular_songs_in_vocab(vocab, converted_data, starting_points, 20)\n",
    "\n",
    "for filename in safe_popular_songs:\n",
    "    start_idx = starting_points[filename]\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d9c279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIDI file written to: generated_music_transformer/popular\\AnotherBrickInTheWall_gen.mid\n",
      "Saved: generated_music_transformer/popular\\AnotherBrickInTheWall_gen.mid\n",
      "MIDI file written to: generated_music_transformer/popular\\BackInBlack_gen.mid\n",
      "Saved: generated_music_transformer/popular\\BackInBlack_gen.mid\n",
      "MIDI file written to: generated_music_transformer/popular\\BillieJean_gen.mid\n",
      "Saved: generated_music_transformer/popular\\BillieJean_gen.mid\n",
      "MIDI file written to: generated_music_transformer/popular\\FinalCountdown_gen.mid\n",
      "Saved: generated_music_transformer/popular\\FinalCountdown_gen.mid\n",
      "MIDI file written to: generated_music_transformer/popular\\Hallelujah_gen.mid\n",
      "Saved: generated_music_transformer/popular\\Hallelujah_gen.mid\n",
      "MIDI file written to: generated_music_transformer/popular\\HappyBirthday_gen.mid\n",
      "Saved: generated_music_transformer/popular\\HappyBirthday_gen.mid\n",
      "MIDI file written to: generated_music_transformer/popular\\HotelCalifornia_gen.mid\n",
      "Saved: generated_music_transformer/popular\\HotelCalifornia_gen.mid\n",
      "MIDI file written to: generated_music_transformer/popular\\Titanic_gen.mid\n",
      "Saved: generated_music_transformer/popular\\Titanic_gen.mid\n",
      "MIDI file written to: generated_music_transformer/popular\\Umbrella_gen.mid\n",
      "Saved: generated_music_transformer/popular\\Umbrella_gen.mid\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"generated_music_transformer/popular\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "context_length = 20\n",
    "generate_events = 50\n",
    "temperature = 0.9\n",
    "\n",
    "\n",
    "for filename in safe_popular_songs:\n",
    "    start_idx = starting_points[filename]\n",
    "    song_data = converted_data[filename]\n",
    "    full_seq = song_data[\"sequence\"]\n",
    "    full_durs = song_data[\"durations\"]\n",
    "\n",
    "    seed_seq = full_seq[start_idx : start_idx + context_length]\n",
    "    seed_durs = full_durs[start_idx : start_idx + context_length]\n",
    "\n",
    "    \n",
    "    if len(seed_seq) != context_length or len(seed_durs) != context_length:\n",
    "        print(f\"Skipping {filename} due to mismatched seed lengths\")\n",
    "        continue\n",
    "\n",
    "    generated_sequence = generate_music(\n",
    "        model=model,\n",
    "        vocab=vocab,\n",
    "        seed_sequence=seed_seq,\n",
    "        seed_durations=seed_durs,\n",
    "        context_length=context_length,\n",
    "        generate_events=generate_events,\n",
    "        temperature=temperature,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    output_path = os.path.join(output_dir, f\"{filename.replace('.mid', '')}_gen.mid\")\n",
    "    sequence_to_midi(generated_sequence, output_path=output_path)\n",
    "\n",
    "    print(f\"Saved: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
