{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88291887",
   "metadata": {},
   "source": [
    "# Data Cleaning and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4cc1ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca60ed6",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da33b846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_parse_all_columns(df):\n",
    "    \"\"\"\n",
    "    Parse all columns in a DataFrame to numeric, coercing errors.\n",
    "    \"\"\"\n",
    "    df['notes'] = df['notes'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    df['chords'] = df['chords'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    df['velocities'] = df['velocities'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    df['durations'] = df['durations'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    df['offsets'] = df['offsets'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    df['ordered_events'] = df['ordered_events'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    return df\n",
    "\n",
    "def load_dataframe_from_two_csvs(file1, file2):\n",
    "    \"\"\"\n",
    "    Load and concatenate two CSV files into a single pandas DataFrame.\n",
    "    \"\"\"\n",
    "    df1 = pd.read_csv(file1)\n",
    "    df2 = pd.read_csv(file2)\n",
    "    full_df = pd.concat([df1, df2], ignore_index=True)\n",
    "    full_df = safe_parse_all_columns(full_df)\n",
    "\n",
    "    return full_df\n",
    "\n",
    "def save_dataframe_to_two_csvs(df, file1, file2):\n",
    "    \"\"\"\n",
    "    Split a DataFrame in half and save it into two CSV files.\n",
    "    \"\"\"\n",
    "    halfway = len(df) // 2\n",
    "    df.iloc[:halfway].to_csv(file1, index=False)\n",
    "    df.iloc[halfway:].to_csv(file2, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e386132",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'data_processed/'\n",
    "file1 = root + 'data_part1.csv'\n",
    "file2 = root + 'data_part2.csv'\n",
    "\n",
    "df = load_dataframe_from_two_csvs(file1, file2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b556ee1f",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f887f3",
   "metadata": {},
   "source": [
    "We first check that there is consistency among the values we have, for example:\n",
    "- The number of notes plus the number of chords should sum to the number of velocities, durations, offsets and events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d106650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Passed: 2775/2775\n",
      " Failed: 0/2775\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_dataset_consistency(df):\n",
    "    failed_rows = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        row = df.iloc[i]\n",
    "        try:\n",
    "            notes = row['notes']\n",
    "            chords = row['chords']\n",
    "            durations = row['durations']\n",
    "            velocities = row['velocities']\n",
    "            offsets = row['offsets']\n",
    "            events = row['ordered_events']\n",
    "\n",
    "            cond1 = len(notes) + len(chords) == len(durations)\n",
    "            cond2 = len(durations) == len(velocities) == len(offsets) == len(events)\n",
    "\n",
    "            if not (cond1 and cond2):\n",
    "                failed_rows.append(i)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing row {i}: {e}\")\n",
    "            failed_rows.append(i)\n",
    "\n",
    "    total = len(df)\n",
    "    failed = len(failed_rows)\n",
    "    passed = total - failed\n",
    "\n",
    "    print(f\"\\n Passed: {passed}/{total}\")\n",
    "    print(f\" Failed: {failed}/{total}\")\n",
    "    if failed > 0:\n",
    "        print(f\"Indices of failed rows: {failed_rows}\")\n",
    "\n",
    "    return failed_rows\n",
    "\n",
    "check_dataset_consistency(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
