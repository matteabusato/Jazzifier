{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation with Variational auto encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we implement and train a Variational Autoencoder (VAE) to generate musical sequences consisting of notes/chords and their corresponding durations. The VAE learns a compressed latent representation of musical patterns and is then used to reconstruct or generate new, stylistically coherent sequences. This allows for creative applications such as \"jazzifying\" existing melodies or generating original music from random latent vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset and Definition of Useful functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'data_processed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_parse_all_columns_df(df):\n",
    "    \"\"\"\n",
    "    Parse all columns in a DataFrame to numeric, coercing errors.\n",
    "    \"\"\"\n",
    "    df['notes'] = df['notes'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    df['chords'] = df['chords'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    df['velocities'] = df['velocities'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    df['durations'] = df['durations'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    df['offsets'] = df['offsets'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    df['ordered_events'] = df['ordered_events'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    return df\n",
    "\n",
    "def load_dataframe_from_two_csvs(file1, file2):\n",
    "    \"\"\"\n",
    "    Load and concatenate two CSV files into a single pandas DataFrame.\n",
    "    \"\"\"\n",
    "    df1 = pd.read_csv(file1)\n",
    "    df2 = pd.read_csv(file2)\n",
    "    full_df = pd.concat([df1, df2], ignore_index=True)\n",
    "    full_df = safe_parse_all_columns_df(full_df)\n",
    "\n",
    "    return full_df\n",
    "\n",
    "def save_dataframe_to_two_csvs(df, file1, file2):\n",
    "    \"\"\"\n",
    "    Split a DataFrame in half and save it into two CSV files.\n",
    "    \"\"\"\n",
    "    halfway = len(df) // 2\n",
    "    df.iloc[:halfway].to_csv(file1, index=False)\n",
    "    df.iloc[halfway:].to_csv(file2, index=False)\n",
    "\n",
    "def load_dataframe_from_one_csv(file):\n",
    "    \"\"\"\n",
    "    Load a DataFrame from a single CSV file.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_dataframe_to_one_csv(df, file):\n",
    "    \"\"\"\n",
    "    Save a DataFrame to a single CSV file.\n",
    "    \"\"\"\n",
    "    df.to_csv(file, index=True)\n",
    "\n",
    "def load_reconstructed_events(file):\n",
    "    \"\"\"\n",
    "    Loads the reconstructed events CSV and safely parses the 'sequence' column,\n",
    "    converting notes to integers and chords to lists of integers.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    def safe_parse(seq_str):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(seq_str)\n",
    "            if not isinstance(parsed, list):\n",
    "                raise ValueError(\"Parsed sequence is not a list\")\n",
    "\n",
    "            normalized = []\n",
    "            for el in parsed:\n",
    "                if isinstance(el, list):\n",
    "                    normalized.append([int(x) for x in el])\n",
    "                else:\n",
    "                    normalized.append(int(el))\n",
    "            return normalized\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing sequence: {seq_str}\")\n",
    "            raise e\n",
    "\n",
    "    df['sequence'] = df['sequence'].apply(safe_parse)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = root + 'data_part1.csv'\n",
    "file2 = root + 'data_part2.csv'\n",
    "\n",
    "df = load_dataframe_from_two_csvs(file1, file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_dataset = load_reconstructed_events(root + 'reconstructed_ordered_events.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_events_with_durations = reconstructed_dataset.copy()\n",
    "ordered_events_with_durations['durations'] = df['durations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sequence</th>\n",
       "      <th>durations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[88, 38, 38, 45, 45, 45, [44, 45], 45, 45, 38,...</td>\n",
       "      <td>[0.25, 2.5, 0.25, 0.3333, 0.3333, 0.25, 0.3333...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[[56, 65, 68, 80, 60], [54, 61, 65, 75], 77, 7...</td>\n",
       "      <td>[1.25, 1.6667, 0.25, 1.0, 1.0, 0.5, 0.6667, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[46, 70, [53, 60, 62], [65, 46, 53, 60], 70, [...</td>\n",
       "      <td>[2.6667, 0.25, 1.25, 0.75, 0.75, 1.6667, 1.75,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[[52, 28, 40], 64, [65, 68], [53, 55, 56], [58...</td>\n",
       "      <td>[3.75, 2.5, 1.0, 2.0, 1.6667, 1.3333, 0.75, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[59, 59, [60, 62, 38, 54], 62, [43, 62, 50, 52...</td>\n",
       "      <td>[0.75, 0.6667, 0.6667, 0.5, 1.0, 0.25, 2.0, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2770</th>\n",
       "      <td>2770</td>\n",
       "      <td>[61, 49, [63, 60], 65, 70, [58, 61, 66, 49], [...</td>\n",
       "      <td>[1.3333, 1.6667, 1.0, 0.5, 0.5, 0.5, 0.6667, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2771</th>\n",
       "      <td>2771</td>\n",
       "      <td>[[45, 55, 67, 69, 72, 75], [46, 74, 56, 62, 68...</td>\n",
       "      <td>[1.0, 1.0, 0.25, 1.25, 0.3333, 0.25, 0.25, 0.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2772</th>\n",
       "      <td>2772</td>\n",
       "      <td>[[45, 57], 71, 52, 61, 64, 45, 69, [45, 69], 4...</td>\n",
       "      <td>[1.6667, 0.6667, 3.3333, 2.75, 2.25, 0.75, 0.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2773</th>\n",
       "      <td>2773</td>\n",
       "      <td>[48, 41, 60, [67, 68, 72, 79], 66, [65, 68, 72...</td>\n",
       "      <td>[0.3333, 0.3333, 0.25, 0.25, 0.3333, 0.3333, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2774</th>\n",
       "      <td>2774</td>\n",
       "      <td>[63, 57, 60, 59, 58, 56, 56, 63, 62, 55, 57, 6...</td>\n",
       "      <td>[2.0, 0.5, 0.25, 0.5, 0.3333, 2.3333, 0.5, 0.6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2775 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                           sequence  \\\n",
       "0         0  [88, 38, 38, 45, 45, 45, [44, 45], 45, 45, 38,...   \n",
       "1         1  [[56, 65, 68, 80, 60], [54, 61, 65, 75], 77, 7...   \n",
       "2         2  [46, 70, [53, 60, 62], [65, 46, 53, 60], 70, [...   \n",
       "3         3  [[52, 28, 40], 64, [65, 68], [53, 55, 56], [58...   \n",
       "4         4  [59, 59, [60, 62, 38, 54], 62, [43, 62, 50, 52...   \n",
       "...     ...                                                ...   \n",
       "2770   2770  [61, 49, [63, 60], 65, 70, [58, 61, 66, 49], [...   \n",
       "2771   2771  [[45, 55, 67, 69, 72, 75], [46, 74, 56, 62, 68...   \n",
       "2772   2772  [[45, 57], 71, 52, 61, 64, 45, 69, [45, 69], 4...   \n",
       "2773   2773  [48, 41, 60, [67, 68, 72, 79], 66, [65, 68, 72...   \n",
       "2774   2774  [63, 57, 60, 59, 58, 56, 56, 63, 62, 55, 57, 6...   \n",
       "\n",
       "                                              durations  \n",
       "0     [0.25, 2.5, 0.25, 0.3333, 0.3333, 0.25, 0.3333...  \n",
       "1     [1.25, 1.6667, 0.25, 1.0, 1.0, 0.5, 0.6667, 0....  \n",
       "2     [2.6667, 0.25, 1.25, 0.75, 0.75, 1.6667, 1.75,...  \n",
       "3     [3.75, 2.5, 1.0, 2.0, 1.6667, 1.3333, 0.75, 0....  \n",
       "4     [0.75, 0.6667, 0.6667, 0.5, 1.0, 0.25, 2.0, 0....  \n",
       "...                                                 ...  \n",
       "2770  [1.3333, 1.6667, 1.0, 0.5, 0.5, 0.5, 0.6667, 0...  \n",
       "2771  [1.0, 1.0, 0.25, 1.25, 0.3333, 0.25, 0.25, 0.7...  \n",
       "2772  [1.6667, 0.6667, 3.3333, 2.75, 2.25, 0.75, 0.5...  \n",
       "2773  [0.3333, 0.3333, 0.25, 0.25, 0.3333, 0.3333, 0...  \n",
       "2774  [2.0, 0.5, 0.25, 0.5, 0.3333, 2.3333, 0.5, 0.6...  \n",
       "\n",
       "[2775 rows x 3 columns]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_events_with_durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All rows have matching 'sequence' and 'durations' lengths.\n"
     ]
    }
   ],
   "source": [
    "mismatches = ordered_events_with_durations.apply(\n",
    "    lambda row: len(row['sequence']) != len(row['durations']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "invalid_rows = ordered_events_with_durations[mismatches]\n",
    "\n",
    "\n",
    "if not invalid_rows.empty:\n",
    "    print(\"Rows with mismatched 'sequence' and 'durations':\")\n",
    "    print(invalid_rows)\n",
    "else:\n",
    "    print(\"All rows have matching 'sequence' and 'durations' lengths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reconstructed = load_reconstructed_events('data_processed/reconstructed_with_durations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reconstructed=ordered_events_with_durations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_chord_to_list(chord):\n",
    "    \"\"\"\n",
    "    Convert a chord string to a list of integers.\n",
    "    \"\"\"\n",
    "    if isinstance(chord, str):\n",
    "        print([int(x) for x in chord.split(',') if x.isdigit()])\n",
    "        return [int(x) for x in chord.split(',') if x.isdigit()]\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_ordered_events(df):\n",
    "    \"\"\"\n",
    "    Reconstruct the ordered list of events (notes and chords) for each song.\n",
    "    \"\"\"\n",
    "    sequences  = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        idx_note = 0\n",
    "        idx_chord = 0\n",
    "        reconstructed = []\n",
    "\n",
    "        for element in df['ordered_events'][i]:\n",
    "            if element == 'n':\n",
    "                reconstructed.append(df['notes'][i][idx_note])\n",
    "                idx_note += 1\n",
    "            elif element == 'c':\n",
    "                parsed_chord = parse_chord_to_list(df['chords'][i][idx_chord])\n",
    "                reconstructed.append(df['chords'][i][idx_chord])\n",
    "                idx_chord += 1\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown event type: {e}\")\n",
    "        \n",
    "        sequences.append(reconstructed)\n",
    "\n",
    "    reconstructed_dataset = pd.DataFrame({'sequence': sequences})\n",
    "    reconstructed_dataset.index.name = 'index'\n",
    "\n",
    "    return reconstructed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sequence</th>\n",
       "      <th>durations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[88, 38, 38, 45, 45, 45, [44, 45], 45, 45, 38,...</td>\n",
       "      <td>[0.25, 2.5, 0.25, 0.3333, 0.3333, 0.25, 0.3333...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[[56, 65, 68, 80, 60], [54, 61, 65, 75], 77, 7...</td>\n",
       "      <td>[1.25, 1.6667, 0.25, 1.0, 1.0, 0.5, 0.6667, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[46, 70, [53, 60, 62], [65, 46, 53, 60], 70, [...</td>\n",
       "      <td>[2.6667, 0.25, 1.25, 0.75, 0.75, 1.6667, 1.75,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[[52, 28, 40], 64, [65, 68], [53, 55, 56], [58...</td>\n",
       "      <td>[3.75, 2.5, 1.0, 2.0, 1.6667, 1.3333, 0.75, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[59, 59, [60, 62, 38, 54], 62, [43, 62, 50, 52...</td>\n",
       "      <td>[0.75, 0.6667, 0.6667, 0.5, 1.0, 0.25, 2.0, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2770</th>\n",
       "      <td>2770</td>\n",
       "      <td>[61, 49, [63, 60], 65, 70, [58, 61, 66, 49], [...</td>\n",
       "      <td>[1.3333, 1.6667, 1.0, 0.5, 0.5, 0.5, 0.6667, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2771</th>\n",
       "      <td>2771</td>\n",
       "      <td>[[45, 55, 67, 69, 72, 75], [46, 74, 56, 62, 68...</td>\n",
       "      <td>[1.0, 1.0, 0.25, 1.25, 0.3333, 0.25, 0.25, 0.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2772</th>\n",
       "      <td>2772</td>\n",
       "      <td>[[45, 57], 71, 52, 61, 64, 45, 69, [45, 69], 4...</td>\n",
       "      <td>[1.6667, 0.6667, 3.3333, 2.75, 2.25, 0.75, 0.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2773</th>\n",
       "      <td>2773</td>\n",
       "      <td>[48, 41, 60, [67, 68, 72, 79], 66, [65, 68, 72...</td>\n",
       "      <td>[0.3333, 0.3333, 0.25, 0.25, 0.3333, 0.3333, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2774</th>\n",
       "      <td>2774</td>\n",
       "      <td>[63, 57, 60, 59, 58, 56, 56, 63, 62, 55, 57, 6...</td>\n",
       "      <td>[2.0, 0.5, 0.25, 0.5, 0.3333, 2.3333, 0.5, 0.6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2775 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                           sequence  \\\n",
       "0         0  [88, 38, 38, 45, 45, 45, [44, 45], 45, 45, 38,...   \n",
       "1         1  [[56, 65, 68, 80, 60], [54, 61, 65, 75], 77, 7...   \n",
       "2         2  [46, 70, [53, 60, 62], [65, 46, 53, 60], 70, [...   \n",
       "3         3  [[52, 28, 40], 64, [65, 68], [53, 55, 56], [58...   \n",
       "4         4  [59, 59, [60, 62, 38, 54], 62, [43, 62, 50, 52...   \n",
       "...     ...                                                ...   \n",
       "2770   2770  [61, 49, [63, 60], 65, 70, [58, 61, 66, 49], [...   \n",
       "2771   2771  [[45, 55, 67, 69, 72, 75], [46, 74, 56, 62, 68...   \n",
       "2772   2772  [[45, 57], 71, 52, 61, 64, 45, 69, [45, 69], 4...   \n",
       "2773   2773  [48, 41, 60, [67, 68, 72, 79], 66, [65, 68, 72...   \n",
       "2774   2774  [63, 57, 60, 59, 58, 56, 56, 63, 62, 55, 57, 6...   \n",
       "\n",
       "                                              durations  \n",
       "0     [0.25, 2.5, 0.25, 0.3333, 0.3333, 0.25, 0.3333...  \n",
       "1     [1.25, 1.6667, 0.25, 1.0, 1.0, 0.5, 0.6667, 0....  \n",
       "2     [2.6667, 0.25, 1.25, 0.75, 0.75, 1.6667, 1.75,...  \n",
       "3     [3.75, 2.5, 1.0, 2.0, 1.6667, 1.3333, 0.75, 0....  \n",
       "4     [0.75, 0.6667, 0.6667, 0.5, 1.0, 0.25, 2.0, 0....  \n",
       "...                                                 ...  \n",
       "2770  [1.3333, 1.6667, 1.0, 0.5, 0.5, 0.5, 0.6667, 0...  \n",
       "2771  [1.0, 1.0, 0.25, 1.25, 0.3333, 0.25, 0.25, 0.7...  \n",
       "2772  [1.6667, 0.6667, 3.3333, 2.75, 2.25, 0.75, 0.5...  \n",
       "2773  [0.3333, 0.3333, 0.25, 0.25, 0.3333, 0.3333, 0...  \n",
       "2774  [2.0, 0.5, 0.25, 0.5, 0.3333, 2.3333, 0.5, 0.6...  \n",
       "\n",
       "[2775 rows x 3 columns]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataframe_to_one_csv((df_reconstructed), root + 'reconstructed_with_durations.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Events (Notes and Chords) and Durations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the data: Fixed number of events "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea for creating the input sequences:\n",
    "- we take subsets of the list of events representing each song \n",
    "- we take the next event of each subset as corresponding training output sequences\n",
    "\n",
    "This is easy to implement and we will have a consistent sequence lenght for batching, but we are ignoring the timing aspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, reconstructed_df):\n",
    "        \"\"\"\n",
    "        Build vocabulary of unique single notes only.\n",
    "        \"\"\"\n",
    "        self.notes = set()\n",
    "        for i in range(len(reconstructed_df)):\n",
    "            sequence = reconstructed_df['sequence'][i]\n",
    "            for event in sequence:\n",
    "                if isinstance(event, list):\n",
    "                    for note in event:\n",
    "                        self.notes.add(note)\n",
    "                else:\n",
    "                    self.notes.add(event)\n",
    "\n",
    "        self.notes = sorted(self.notes)\n",
    "        self.note_to_idx = {note: idx for idx, note in enumerate(self.notes)}\n",
    "        self.idx_to_note = {idx: note for idx, note in enumerate(self.notes)}\n",
    "        self.vocab_size = len(self.notes)\n",
    "\n",
    "    def encode_event(self, event):\n",
    "        \"\"\"\n",
    "        Encode an event as a multi-hot vector over single notes.\n",
    "        \"\"\"\n",
    "        vec = np.zeros(self.vocab_size, dtype=np.float32)\n",
    "        if isinstance(event, list):\n",
    "            for note in event:\n",
    "                vec[self.note_to_idx[note]] = 1.0\n",
    "        else:\n",
    "            vec[self.note_to_idx[event]] = 1.0\n",
    "        return vec\n",
    "\n",
    "    def decode_event(self, vec, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Decode multi-hot vector to list of notes.\n",
    "        \"\"\"\n",
    "        indices = np.where(vec >= threshold)[0]\n",
    "        notes = [self.idx_to_note[idx] for idx in indices]\n",
    "        if len(notes) == 1:\n",
    "            return notes[0]\n",
    "        else:\n",
    "            return notes\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.vocab_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we consider also the duration while creating the dataset, we return two targets independently, related to durations and events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicEventDataset(Dataset):\n",
    "    def __init__(self, reconstructed_df, vocab, seq_length=50, max_samples_per_song=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            reconstructed_df: DataFrame with 'sequence' and 'durations' columns\n",
    "            vocab: Vocabulary object to encode events\n",
    "            seq_length: Input sequence length\n",
    "            max_samples_per_song: Max number of (input, target) pairs to keep per song\n",
    "        \"\"\"\n",
    "        self.samples = []\n",
    "        self.seq_length = seq_length\n",
    "        self.vocab = vocab\n",
    "\n",
    "        for row_index in range(len(reconstructed_df)):\n",
    "            sequence = reconstructed_df['sequence'][row_index]\n",
    "            durations = reconstructed_df['durations'][row_index]\n",
    "\n",
    "            if isinstance(durations, str):\n",
    "                durations = ast.literal_eval(durations)\n",
    "\n",
    "            n_events = min(len(sequence), len(durations))\n",
    "            if n_events <= seq_length:\n",
    "                continue\n",
    "\n",
    "            num_added = 0\n",
    "            for i in range(n_events - seq_length):\n",
    "                if max_samples_per_song is not None and num_added >= max_samples_per_song:\n",
    "                    break\n",
    "\n",
    "                input_seq = sequence[i : i + seq_length]\n",
    "                input_durs = durations[i : i + seq_length]\n",
    "                target_event = sequence[i + seq_length]\n",
    "                target_dur = durations[i + seq_length]\n",
    "\n",
    "                self.samples.append((input_seq, input_durs, target_event, target_dur))\n",
    "                num_added += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seq, input_durs, target_event, target_dur = self.samples[idx]\n",
    "\n",
    "        input_encoded = [self.vocab.encode_event(event) for event in input_seq]\n",
    "        input_tensor = torch.tensor(input_encoded, dtype=torch.float32)\n",
    "\n",
    "        dur_tensor = torch.tensor(input_durs, dtype=torch.float32).unsqueeze(-1)\n",
    "\n",
    "        target_encoded = self.vocab.encode_event(target_event)\n",
    "        target_tensor = torch.tensor(target_encoded, dtype=torch.float32)\n",
    "\n",
    "        target_dur_tensor = torch.tensor([target_dur], dtype=torch.float32)\n",
    "\n",
    "        return input_tensor, dur_tensor, target_tensor, target_dur_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_dataset = load_reconstructed_events(root + 'reconstructed_ordered_events.csv')\n",
    "vocab = Vocabulary(reconstructed_dataset)\n",
    "dataset = MusicEventDataset(ordered_events_with_durations, vocab=vocab, seq_length=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we show how the first song is enconded, only the first 16 events. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence shape: torch.Size([16, 88])\n",
      "Duration sequence shape: torch.Size([16, 1])\n",
      "Next event shape: torch.Size([88])\n",
      "Target duration shape: torch.Size([1])\n",
      "Input sequence (multi-hot vectors):\n",
      " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Duration sequence:\n",
      " tensor([[0.2500],\n",
      "        [2.5000],\n",
      "        [0.2500],\n",
      "        [0.3333],\n",
      "        [0.3333],\n",
      "        [0.2500],\n",
      "        [0.3333],\n",
      "        [0.2500],\n",
      "        [0.3333],\n",
      "        [0.7500],\n",
      "        [1.5000],\n",
      "        [0.3333],\n",
      "        [0.2500],\n",
      "        [1.0000],\n",
      "        [0.3333],\n",
      "        [0.3333]])\n",
      "Next event (multi-hot vector): tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Target duration: tensor([1.7500])\n"
     ]
    }
   ],
   "source": [
    "x, dur, y, target_dur = dataset[0]\n",
    "\n",
    "print(\"Input sequence shape:\", x.shape)\n",
    "print(\"Duration sequence shape:\", dur.shape)\n",
    "print(\"Next event shape:\", y.shape)\n",
    "print(\"Target duration shape:\", target_dur.shape)\n",
    "\n",
    "print(\"Input sequence (multi-hot vectors):\\n\", x)\n",
    "print(\"Duration sequence:\\n\", dur)\n",
    "print(\"Next event (multi-hot vector):\", y)\n",
    "print(\"Target duration:\", target_dur)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Variational Autoencoder for music sequence modeling with separate encoders\n",
    "    for note/chord events and durations.\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): Size of the input feature dimension (e.g., 88 for piano roll).\n",
    "        latent_dim (int): Dimension of the latent space.\n",
    "        seq_length (int): Length of the input sequence.\n",
    "\n",
    "    Architecture:\n",
    "        - Encodes both event sequences and duration sequences.\n",
    "        - Projects to latent space with reparameterization trick.\n",
    "        - Decodes back to event predictions (multi-hot) and duration values (scalar).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, latent_dim, seq_length):\n",
    "        super(VAE, self).__init__()\n",
    "        self.seq_length = seq_length\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim * seq_length, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.duration_encoder = nn.Sequential(\n",
    "            nn.Linear(seq_length, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.fc_mu = nn.Linear(256 + 64, latent_dim)\n",
    "        self.fc_var = nn.Linear(256 + 64, latent_dim)\n",
    "\n",
    "        self.event_decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, input_dim),\n",
    "            nn.Sigmoid(),  \n",
    "        )\n",
    "\n",
    "        self.duration_decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def encode(self, x, durs):\n",
    "        h_seq = self.encoder(x.view(-1, self.seq_length * x.size(-1)))\n",
    "        h_dur = self.duration_encoder(durs.view(-1, self.seq_length))\n",
    "        h = torch.cat((h_seq, h_dur), dim=1)\n",
    "        mu = self.fc_mu(h)\n",
    "        log_var = self.fc_var(h)\n",
    "        return mu, log_var\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        event = self.event_decoder(z)\n",
    "        duration = self.duration_decoder(z)\n",
    "        return event, duration\n",
    "\n",
    "    def forward(self, x, durs):\n",
    "        mu, log_var = self.encode(x, durs)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return self.decode(z), mu, log_var\n",
    "\n",
    "input_dim = 88  \n",
    "latent_dim = 20\n",
    "seq_length = 16\n",
    "\n",
    "vae = VAE(input_dim, latent_dim, seq_length).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "input_dim = 88  \n",
    "latent_dim = 20\n",
    "seq_length = 16\n",
    "vae = VAE(input_dim, latent_dim, seq_length).to(device)\n",
    "\n",
    "optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
    "\n",
    "def loss_function(recon_event, event, recon_dur, dur, mu, log_var):\n",
    "    \"\"\"\n",
    "    Computes the total VAE loss: reconstruction loss for events and durations,\n",
    "    plus the KL divergence between approximate posterior and prior.\n",
    "\n",
    "    Args:\n",
    "        recon_event (Tensor): Predicted event output [B, input_dim]\n",
    "        event (Tensor): Ground truth event [B, input_dim]\n",
    "        recon_dur (Tensor): Predicted duration output [B, 1]\n",
    "        dur (Tensor): Ground truth duration [B, 1]\n",
    "        mu (Tensor): Latent mean [B, latent_dim]\n",
    "        log_var (Tensor): Latent log variance [B, latent_dim]\n",
    "\n",
    "    Returns:\n",
    "        total_loss (Tensor): Scalar tensor representing total loss\n",
    "    \"\"\"\n",
    "    recon_loss_event = F.binary_cross_entropy(recon_event, event, reduction='sum')\n",
    "    recon_loss_dur = F.mse_loss(recon_dur, dur, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return recon_loss_event + recon_loss_dur + KLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 0/139758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 2000/139758\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[227], line 26\u001b[0m\n\u001b[1;32m     22\u001b[0m recon_event, recon_dur \u001b[38;5;241m=\u001b[39m recon_batch\n\u001b[1;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(recon_event, target_event, recon_dur, target_dur, mu, log_var)\n\u001b[0;32m---> 26\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \"\"\"\n",
    "    Training loop for the Variational Autoencoder (VAE).\n",
    "    Each epoch iterates through the entire dataset using batches.\n",
    "    The model learns to reconstruct musical events and durations while regularizing its latent space.\n",
    "    \"\"\"\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch_idx, (input_seq, dur_seq, target_event, target_dur) in enumerate(data_loader):\n",
    "\n",
    "\n",
    "        input_seq = input_seq.to(device)\n",
    "        dur_seq = dur_seq.to(device)\n",
    "        target_event = target_event.to(device)\n",
    "        target_dur = target_dur.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        recon_batch, mu, log_var = vae(input_seq, dur_seq)\n",
    "        recon_event, recon_dur = recon_batch\n",
    "\n",
    "        loss = loss_function(recon_event, target_event, recon_dur, target_dur, mu, log_var)\n",
    "\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 2000 == 0:\n",
    "            print(f'batch: {batch_idx}/{len(data_loader)}')\n",
    "\n",
    "    print(f'Epoch: {epoch+1}, Loss: {train_loss / len(data_loader.dataset)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vae.state_dict(), 'vae_model.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a smaller data set for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_events_with_durations_short = ordered_events_with_durations.iloc[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MusicEventDataset(\n",
    "    reconstructed_df=ordered_events_with_durations,\n",
    "    vocab=vocab,\n",
    "    seq_length=16,\n",
    "    max_samples_per_song=15  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 9.482798246243998\n",
      "Epoch 1: Recon loss = 232.11, KL = 1.08, Total loss = 256.84\n",
      "Epoch: 2, Loss: 9.479013233305787\n",
      "Epoch 2: Recon loss = 241.69, KL = 0.58, Total loss = 291.27\n",
      "Epoch: 3, Loss: 9.493823725055003\n",
      "Epoch 3: Recon loss = 215.61, KL = 0.64, Total loss = 273.34\n",
      "Epoch: 4, Loss: 9.497717188524096\n",
      "Epoch 4: Recon loss = 248.54, KL = 1.88, Total loss = 286.12\n",
      "Epoch: 5, Loss: 9.498617762078602\n",
      "Epoch 5: Recon loss = 243.18, KL = 0.64, Total loss = 273.20\n",
      "Epoch: 6, Loss: 9.50224819253001\n",
      "Epoch 6: Recon loss = 218.38, KL = 1.09, Total loss = 260.85\n",
      "Epoch: 7, Loss: 9.505693193355386\n",
      "Epoch 7: Recon loss = 218.73, KL = 1.09, Total loss = 288.14\n",
      "Epoch: 8, Loss: 9.511159260823318\n",
      "Epoch 8: Recon loss = 289.76, KL = 0.84, Total loss = 360.55\n",
      "Epoch: 9, Loss: 9.508938914190319\n",
      "Epoch 9: Recon loss = 268.31, KL = 0.40, Total loss = 322.76\n",
      "Epoch: 10, Loss: 9.523902287099217\n",
      "Epoch 10: Recon loss = 275.80, KL = 1.16, Total loss = 315.78\n",
      "Epoch: 11, Loss: 9.518570483338024\n",
      "Epoch 11: Recon loss = 258.09, KL = 0.86, Total loss = 291.09\n",
      "Epoch: 12, Loss: 9.51557614027526\n",
      "Epoch 12: Recon loss = 235.00, KL = 1.22, Total loss = 280.05\n",
      "Epoch: 13, Loss: 9.519233256744554\n",
      "Epoch 13: Recon loss = 209.23, KL = 0.81, Total loss = 261.90\n",
      "Epoch: 14, Loss: 9.518083370765218\n",
      "Epoch 14: Recon loss = 243.95, KL = 0.49, Total loss = 272.96\n",
      "Epoch: 15, Loss: 9.51883211196351\n",
      "Epoch 15: Recon loss = 243.09, KL = 0.62, Total loss = 291.89\n",
      "Epoch: 16, Loss: 9.522828055738646\n",
      "Epoch 16: Recon loss = 194.14, KL = 0.97, Total loss = 345.59\n",
      "Epoch: 17, Loss: 9.51876305259926\n",
      "Epoch 17: Recon loss = 239.97, KL = 0.75, Total loss = 261.44\n",
      "Epoch: 18, Loss: 9.52186869750132\n",
      "Epoch 18: Recon loss = 273.11, KL = 0.82, Total loss = 308.06\n",
      "Epoch: 19, Loss: 9.520477642079122\n",
      "Epoch 19: Recon loss = 211.50, KL = 0.87, Total loss = 269.73\n",
      "Epoch: 20, Loss: 9.519686385298975\n",
      "Epoch 20: Recon loss = 211.96, KL = 1.41, Total loss = 285.98\n"
     ]
    }
   ],
   "source": [
    "num_epochs=20\n",
    "for epoch in range(num_epochs):\n",
    "    \"\"\"\n",
    "    Training loop with KL annealing for the Variational Autoencoder (VAE).\n",
    "    KL weight (beta) increases linearly from 0 to 1 over the first 10 epochs.\n",
    "    \"\"\"\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    beta = min(1.0, epoch / 10)  \n",
    "\n",
    "    for batch_idx, (input_seq, dur_seq, target_event, target_dur) in enumerate(data_loader):\n",
    "        input_seq = input_seq.to(device)\n",
    "        dur_seq = dur_seq.to(device)\n",
    "        target_event = target_event.to(device)\n",
    "        target_dur = target_dur.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, log_var = vae(input_seq, dur_seq)\n",
    "        recon_event, recon_dur = recon_batch\n",
    "\n",
    "        recon_loss_event = F.binary_cross_entropy(recon_event, target_event, reduction='sum')\n",
    "        recon_loss_dur = F.mse_loss(recon_dur, target_dur, reduction='sum')\n",
    "        kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "\n",
    "        loss = recon_loss_event + recon_loss_dur + beta * kl_loss\n",
    "\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    print(f'Epoch: {epoch+1}, Loss: {train_loss / len(data_loader.dataset)}')\n",
    "    print(f\"Epoch {epoch+1}: Recon loss = {recon_loss_event.item():.2f}, KL = {kl_loss.item():.2f}, Total loss = {loss.item():.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vae.state_dict(), 'vae_model_KLdivsmalldata.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning for smaller models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna hyperparameter tuning.\n",
    "\n",
    "    Args:\n",
    "        trial (optuna.trial.Trial): A single trial run to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        float: Average loss over 20 batches for the current hyperparameter setting.\n",
    "    \"\"\"\n",
    "    latent_dim = trial.suggest_categorical('latent_dim', [8, 16, 32])\n",
    "    hidden_size = trial.suggest_categorical('hidden_size', [128, 256, 512])\n",
    "    lr = trial.suggest_loguniform('lr', 1e-4, 1e-2)\n",
    "\n",
    "    model = VAE(input_dim=88, latent_dim=latent_dim, seq_length=16).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for batch_idx, (x, dur, y, y_dur) in enumerate(data_loader):\n",
    "        x, dur, y, y_dur = x.to(device), dur.to(device), y.to(device), y_dur.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon, mu, log_var = model(x, dur)\n",
    "        recon_event, recon_dur = recon\n",
    "        loss = loss_function(recon_event, y, recon_dur, y_dur, mu, log_var)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if batch_idx > 20:\n",
    "            break  \n",
    "\n",
    "    return total_loss / 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-01 18:17:11,239] A new study created in memory with name: no-name-7a9213ab-5a39-4c7c-87cd-359a4f79099a\n",
      "/var/folders/69/hhjjr06524j6ztntghhvpzk80000gn/T/ipykernel_16947/3701089035.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform('lr', 1e-4, 1e-2)\n",
      "[I 2025-06-01 18:17:12,344] Trial 0 finished with value: 1909.0768676757812 and parameters: {'latent_dim': 16, 'hidden_size': 128, 'lr': 0.00023143635707429156}. Best is trial 0 with value: 1909.0768676757812.\n",
      "[I 2025-06-01 18:17:13,509] Trial 1 finished with value: 663.9562545776367 and parameters: {'latent_dim': 8, 'hidden_size': 512, 'lr': 0.0024741274585548}. Best is trial 1 with value: 663.9562545776367.\n",
      "[I 2025-06-01 18:17:14,334] Trial 2 finished with value: 595.090219116211 and parameters: {'latent_dim': 32, 'hidden_size': 128, 'lr': 0.0036996539293656532}. Best is trial 2 with value: 595.090219116211.\n",
      "[I 2025-06-01 18:17:15,212] Trial 3 finished with value: 1104.8008758544922 and parameters: {'latent_dim': 8, 'hidden_size': 256, 'lr': 0.0007163988662801532}. Best is trial 2 with value: 595.090219116211.\n",
      "[I 2025-06-01 18:17:16,151] Trial 4 finished with value: 2107.4494445800783 and parameters: {'latent_dim': 16, 'hidden_size': 512, 'lr': 0.00010439890659726781}. Best is trial 2 with value: 595.090219116211.\n",
      "[I 2025-06-01 18:17:16,989] Trial 5 finished with value: 649.6262741088867 and parameters: {'latent_dim': 8, 'hidden_size': 512, 'lr': 0.0028474763488817853}. Best is trial 2 with value: 595.090219116211.\n",
      "[I 2025-06-01 18:17:17,797] Trial 6 finished with value: 792.6088821411133 and parameters: {'latent_dim': 8, 'hidden_size': 128, 'lr': 0.0017691683837948603}. Best is trial 2 with value: 595.090219116211.\n",
      "[I 2025-06-01 18:17:18,727] Trial 7 finished with value: 867.3584014892579 and parameters: {'latent_dim': 32, 'hidden_size': 256, 'lr': 0.001198527691879992}. Best is trial 2 with value: 595.090219116211.\n",
      "[I 2025-06-01 18:17:20,231] Trial 8 finished with value: 723.9919082641602 and parameters: {'latent_dim': 8, 'hidden_size': 128, 'lr': 0.0016251330657141528}. Best is trial 2 with value: 595.090219116211.\n",
      "[I 2025-06-01 18:17:21,160] Trial 9 finished with value: 1198.0648849487304 and parameters: {'latent_dim': 8, 'hidden_size': 512, 'lr': 0.0005850108752634055}. Best is trial 2 with value: 595.090219116211.\n",
      "[I 2025-06-01 18:17:22,701] Trial 10 finished with value: 579.6352645874024 and parameters: {'latent_dim': 32, 'hidden_size': 128, 'lr': 0.008879469202908307}. Best is trial 10 with value: 579.6352645874024.\n",
      "[I 2025-06-01 18:17:23,526] Trial 11 finished with value: 603.0780624389648 and parameters: {'latent_dim': 32, 'hidden_size': 128, 'lr': 0.003968213469608066}. Best is trial 10 with value: 579.6352645874024.\n",
      "[I 2025-06-01 18:17:24,290] Trial 12 finished with value: 562.5059875488281 and parameters: {'latent_dim': 32, 'hidden_size': 128, 'lr': 0.009797985153518709}. Best is trial 12 with value: 562.5059875488281.\n",
      "[I 2025-06-01 18:17:25,054] Trial 13 finished with value: 567.8250778198242 and parameters: {'latent_dim': 32, 'hidden_size': 128, 'lr': 0.00944284252094676}. Best is trial 12 with value: 562.5059875488281.\n",
      "[I 2025-06-01 18:17:25,798] Trial 14 finished with value: 539.3251754760743 and parameters: {'latent_dim': 32, 'hidden_size': 128, 'lr': 0.009229601383941896}. Best is trial 14 with value: 539.3251754760743.\n",
      "[I 2025-06-01 18:17:26,572] Trial 15 finished with value: 612.1003280639649 and parameters: {'latent_dim': 32, 'hidden_size': 128, 'lr': 0.0059300197332981894}. Best is trial 14 with value: 539.3251754760743.\n",
      "[I 2025-06-01 18:17:27,637] Trial 16 finished with value: 572.2220932006836 and parameters: {'latent_dim': 32, 'hidden_size': 256, 'lr': 0.006289828413230651}. Best is trial 14 with value: 539.3251754760743.\n",
      "[I 2025-06-01 18:17:28,387] Trial 17 finished with value: 610.8965454101562 and parameters: {'latent_dim': 32, 'hidden_size': 128, 'lr': 0.004799231008627704}. Best is trial 14 with value: 539.3251754760743.\n",
      "[I 2025-06-01 18:17:29,561] Trial 18 finished with value: 549.2381790161132 and parameters: {'latent_dim': 16, 'hidden_size': 128, 'lr': 0.00994163891076651}. Best is trial 14 with value: 539.3251754760743.\n",
      "[I 2025-06-01 18:17:30,524] Trial 19 finished with value: 1487.5218536376954 and parameters: {'latent_dim': 16, 'hidden_size': 256, 'lr': 0.000425005856073929}. Best is trial 14 with value: 539.3251754760743.\n",
      "[I 2025-06-01 18:17:31,326] Trial 20 finished with value: 702.7796951293946 and parameters: {'latent_dim': 16, 'hidden_size': 128, 'lr': 0.0024031440069391756}. Best is trial 14 with value: 539.3251754760743.\n",
      "[I 2025-06-01 18:17:32,119] Trial 21 finished with value: 527.0857391357422 and parameters: {'latent_dim': 16, 'hidden_size': 128, 'lr': 0.00950663997831853}. Best is trial 21 with value: 527.0857391357422.\n",
      "[I 2025-06-01 18:17:33,021] Trial 22 finished with value: 592.112158203125 and parameters: {'latent_dim': 16, 'hidden_size': 128, 'lr': 0.005893249736773069}. Best is trial 21 with value: 527.0857391357422.\n",
      "[I 2025-06-01 18:17:33,832] Trial 23 finished with value: 526.7292922973633 and parameters: {'latent_dim': 16, 'hidden_size': 128, 'lr': 0.007322621104329893}. Best is trial 23 with value: 526.7292922973633.\n",
      "[I 2025-06-01 18:17:34,942] Trial 24 finished with value: 580.960041809082 and parameters: {'latent_dim': 16, 'hidden_size': 128, 'lr': 0.00633049451633444}. Best is trial 23 with value: 526.7292922973633.\n",
      "[I 2025-06-01 18:17:35,949] Trial 25 finished with value: 613.626953125 and parameters: {'latent_dim': 16, 'hidden_size': 128, 'lr': 0.003522959386187989}. Best is trial 23 with value: 526.7292922973633.\n",
      "[I 2025-06-01 18:17:37,756] Trial 26 finished with value: 613.1325042724609 and parameters: {'latent_dim': 16, 'hidden_size': 128, 'lr': 0.004885238942194763}. Best is trial 23 with value: 526.7292922973633.\n",
      "[I 2025-06-01 18:17:38,556] Trial 27 finished with value: 582.0913436889648 and parameters: {'latent_dim': 16, 'hidden_size': 128, 'lr': 0.007377428601725233}. Best is trial 23 with value: 526.7292922973633.\n",
      "[I 2025-06-01 18:17:39,321] Trial 28 finished with value: 755.5218658447266 and parameters: {'latent_dim': 16, 'hidden_size': 512, 'lr': 0.0019572099145520132}. Best is trial 23 with value: 526.7292922973633.\n",
      "[I 2025-06-01 18:17:40,094] Trial 29 finished with value: 2135.1812072753905 and parameters: {'latent_dim': 16, 'hidden_size': 256, 'lr': 0.00015995912295754808}. Best is trial 23 with value: 526.7292922973633.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'latent_dim': 16, 'hidden_size': 128, 'lr': 0.007322621104329893}\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=30)\n",
    "print(\"Best hyperparameters:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(\n",
    "    input_dim=88,\n",
    "    latent_dim=16,\n",
    "    seq_length=16,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=0.0073)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 259232335.18434486\n",
      "Epoch 1: Recon loss = 241.23, KL = 30233.47, Total loss = 285.48\n",
      "Epoch: 2, Loss: 2.47310193869874e+22\n",
      "Epoch 2: Recon loss = 223.20, KL = 49445.65, Total loss = 9210.66\n",
      "Epoch: 3, Loss: 2.7958120468247243e+24\n",
      "Epoch 3: Recon loss = 232.35, KL = 60712.52, Total loss = 13209.80\n",
      "Epoch: 4, Loss: 2.1955094555948867e+24\n",
      "Epoch 4: Recon loss = 238.70, KL = 26403.71, Total loss = 8385.95\n",
      "Epoch: 5, Loss: 3.4229836111250214e+25\n",
      "Epoch 5: Recon loss = 199.98, KL = 149801.64, Total loss = 117298.65\n",
      "Epoch: 6, Loss: 4.836802302492654e+26\n",
      "Epoch 6: Recon loss = 235.46, KL = 157569.31, Total loss = 92288.76\n",
      "Epoch: 7, Loss: 4.780526032072211e+19\n",
      "Epoch 7: Recon loss = 253.00, KL = 305095.62, Total loss = 282386.69\n",
      "Epoch: 8, Loss: 3.1706394544881197e+19\n",
      "Epoch 8: Recon loss = 273.93, KL = 270564.97, Total loss = 407648.75\n",
      "Epoch: 9, Loss: 1.9492545171726975e+19\n",
      "Epoch 9: Recon loss = 256.39, KL = 523088.12, Total loss = 486682.78\n",
      "Epoch: 10, Loss: 2.1606448106054218e+19\n",
      "Epoch 10: Recon loss = 201.75, KL = 167865.19, Total loss = 178528.67\n",
      "Epoch: 11, Loss: 1.2128834496959357e+20\n",
      "Epoch 11: Recon loss = 227.42, KL = 269082.94, Total loss = 308200.31\n",
      "Epoch: 12, Loss: 3.208933647535653e+20\n",
      "Epoch 12: Recon loss = 216.73, KL = 139014.23, Total loss = 158615.09\n",
      "Epoch: 13, Loss: 1.1970068198229441e+20\n",
      "Epoch 13: Recon loss = 217.50, KL = 149051.22, Total loss = 212147.69\n",
      "Epoch: 14, Loss: 2.5273399930287927e+21\n",
      "Epoch 14: Recon loss = 257.33, KL = 150125.81, Total loss = 167698.58\n",
      "Epoch: 15, Loss: 1.0628551902477045e+20\n",
      "Epoch 15: Recon loss = 195.36, KL = 243540.06, Total loss = 301979.97\n",
      "Epoch: 16, Loss: 7.38152398231715e+20\n",
      "Epoch 16: Recon loss = 233.40, KL = 1156943.50, Total loss = 2302597.50\n",
      "Epoch: 17, Loss: 1.8716440601843506e+27\n",
      "Epoch 17: Recon loss = 249.52, KL = 1916134.25, Total loss = 3664477.00\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "all elements of input should be between 0 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[249], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m recon_batch, mu, log_var \u001b[38;5;241m=\u001b[39m vae(input_seq, dur_seq)\n\u001b[1;32m     19\u001b[0m recon_event, recon_dur \u001b[38;5;241m=\u001b[39m recon_batch\n\u001b[0;32m---> 21\u001b[0m recon_loss_event \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecon_event\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_event\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m recon_loss_dur \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(recon_dur, target_dur, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     23\u001b[0m kl_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m log_var \u001b[38;5;241m-\u001b[39m mu\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m-\u001b[39m log_var\u001b[38;5;241m.\u001b[39mexp())\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/functional.py:3127\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3124\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m   3125\u001b[0m     weight \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mexpand(new_size)\n\u001b[0;32m-> 3127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: all elements of input should be between 0 and 1"
     ]
    }
   ],
   "source": [
    "num_epochs=20\n",
    "for epoch in range(num_epochs):\n",
    "    \"\"\"\n",
    "    Training loop for VAE with KL annealing.\n",
    "    The KL term is weighted by Î², which increases linearly from 0 to 1 by epoch 10.\n",
    "    \"\"\"\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    beta = min(1.0, epoch / 10)  \n",
    "\n",
    "    for batch_idx, (input_seq, dur_seq, target_event, target_dur) in enumerate(data_loader):\n",
    "        input_seq = input_seq.to(device)\n",
    "        dur_seq = dur_seq.to(device)\n",
    "        target_event = target_event.to(device)\n",
    "        target_dur = target_dur.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, log_var = vae(input_seq, dur_seq)\n",
    "        recon_event, recon_dur = recon_batch\n",
    "\n",
    "        recon_loss_event = F.binary_cross_entropy(recon_event, target_event, reduction='sum')\n",
    "        recon_loss_dur = F.mse_loss(recon_dur, target_dur, reduction='sum')\n",
    "        kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "\n",
    "        loss = recon_loss_event + recon_loss_dur + beta * kl_loss\n",
    "\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    print(f'Epoch: {epoch+1}, Loss: {train_loss / len(data_loader.dataset)}')\n",
    "    print(f\"Epoch {epoch+1}: Recon loss = {recon_loss_event.item():.2f}, KL = {kl_loss.item():.2f}, Total loss = {loss.item():.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vae.state_dict(), 'vae_model_hyperparameterKL.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose to both generate random sequences of notes as well as trying to jazzify popular songs. We also retrieve some main evaluation measures in order to picture how our model works. The genrated songs are all saved can all be listened to to perform qualitative evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random generated output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start off by generating random outputs to check their format is coherent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_dataset = Subset(dataset, range(100))  \n",
    "\n",
    "val_loader = DataLoader(subset_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluates the VAE model on the given DataLoader.\n",
    "\n",
    "    Args:\n",
    "        model: Trained VAE model\n",
    "        data_loader: DataLoader for validation/test set\n",
    "        device: 'cuda' or 'cpu'\n",
    "    \n",
    "    Returns:\n",
    "        avg_event_loss: Average binary cross-entropy loss for event reconstruction\n",
    "        avg_dur_loss: Average MSE loss for duration reconstruction\n",
    "        avg_kl_loss: Average KL divergence\n",
    "        avg_total_loss: Sum of all components per sample\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    total_recon_event_loss = 0\n",
    "    total_recon_dur_loss = 0\n",
    "    total_kl_loss = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_seq, dur_seq, target_event, target_dur in data_loader:\n",
    "            input_seq = input_seq.to(device)\n",
    "            dur_seq = dur_seq.to(device)\n",
    "            target_event = target_event.to(device)\n",
    "            target_dur = target_dur.to(device)\n",
    "\n",
    "            (recon_event, recon_dur), mu, log_var = model(input_seq, dur_seq)\n",
    "\n",
    "            recon_loss_event = F.binary_cross_entropy(recon_event, target_event, reduction='sum')\n",
    "            recon_loss_dur = F.mse_loss(recon_dur, target_dur, reduction='sum')\n",
    "\n",
    "            kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "\n",
    "            total_recon_event_loss += recon_loss_event.item()\n",
    "            total_recon_dur_loss += recon_loss_dur.item()\n",
    "            total_kl_loss += kl_loss.item()\n",
    "            total_samples += input_seq.size(0)\n",
    "\n",
    "    avg_event_loss = total_recon_event_loss / total_samples\n",
    "    avg_dur_loss = total_recon_dur_loss / total_samples\n",
    "    avg_kl_loss = total_kl_loss / total_samples\n",
    "    avg_total_loss = (total_recon_event_loss + total_recon_dur_loss + total_kl_loss) / total_samples\n",
    "\n",
    "    print(f\"Evaluation Results:\")\n",
    "    print(f\"Event BCE Loss:   {avg_event_loss:.4f}\")\n",
    "    print(f\"Duration MSE Loss:{avg_dur_loss:.4f}\")\n",
    "    print(f\"KL Divergence:    {avg_kl_loss:.4f}\")\n",
    "    print(f\"Total Loss:       {avg_total_loss:.4f}\")\n",
    "\n",
    "    return avg_event_loss, avg_dur_loss, avg_kl_loss, avg_total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random (model, latent_dim, seq_length):\n",
    "    \"\"\"\n",
    "    Generates a random jazzified sequence from a trained VAE model by sampling from the latent space.\n",
    "\n",
    "    Args:\n",
    "        model: Trained VAE model\n",
    "        latent_dim: Dimensionality of latent space\n",
    "        seq_length: Number of events to generate (not used internally but useful for control)\n",
    "\n",
    "    Returns:\n",
    "        generated_sequence: List of decoded musical events (notes/chords)\n",
    "        generated_durations: List of durations for each event\n",
    "    \"\"\"\n",
    "    generated_sequence = []\n",
    "    generated_durations = []\n",
    "\n",
    "    top_k = 5  \n",
    "\n",
    "\n",
    "    for _ in range(32):  \n",
    "        z = torch.randn(1, latent_dim).to(device)\n",
    "        recon_event, recon_dur = vae.decode(z)\n",
    "\n",
    "        event_vec = recon_event.squeeze().detach().cpu().numpy()\n",
    "        duration_val = recon_dur.item()\n",
    "\n",
    "        top_indices = np.argsort(event_vec)[-top_k:]\n",
    "        decoded_event = [vocab.idx_to_note[idx] for idx in top_indices if event_vec[idx] > 0.05]\n",
    "\n",
    "        if not decoded_event:\n",
    "            decoded_event = [vocab.idx_to_note[np.argmax(event_vec)]]\n",
    "\n",
    "        generated_sequence.append(decoded_event)\n",
    "        generated_durations.append(duration_val)\n",
    "\n",
    "    print(\"Generated sequence:\", generated_sequence)\n",
    "    print(\"Durations:\", generated_durations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from music21 import converter, note, chord\n",
    "\n",
    "\n",
    "def set_seed(seed=16):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "def parse_midi_to_event_sequence(midi_path):\n",
    "    \"\"\"\n",
    "    Parses a MIDI file into a list of events (note or chord),\n",
    "    their durations (in quarterLength), and velocities.\n",
    "\n",
    "    Returns:\n",
    "        events: List of MIDI pitch numbers or lists of pitch numbers (for chords)\n",
    "        durations: List of corresponding durations (floats)\n",
    "        velocities: List of velocity values (int, 0â€“127)\n",
    "    \"\"\"\n",
    "    score = converter.parse(midi_path)\n",
    "    flat = score.flat.notes\n",
    "    events = []\n",
    "    durations = []\n",
    "    velocities = []\n",
    "\n",
    "    for el in flat:\n",
    "        if isinstance(el, note.Note):\n",
    "            events.append(el.pitch.midi)\n",
    "            durations.append(el.duration.quarterLength)\n",
    "            velocities.append(el.volume.velocity if el.volume.velocity is not None else 64)\n",
    "        elif isinstance(el, chord.Chord):\n",
    "            events.append([p.midi for p in el.pitches])\n",
    "            durations.append(el.duration.quarterLength)\n",
    "            velocities.append(el.volume.velocity if el.volume.velocity is not None else 64)\n",
    "\n",
    "    return events, durations, velocities\n",
    "\n",
    "def encode_input_sequence(melody, durations, seq_length, vocab):\n",
    "    \"\"\"\n",
    "    Encodes a melody and its corresponding durations into tensor sequences suitable for VAE input.\n",
    "\n",
    "    Args:\n",
    "        melody: List of note pitches (e.g., MIDI numbers or chord representations)\n",
    "        durations: List of durations (floats), aligned with the melody\n",
    "        seq_length: Length of each input sequence (number of time steps)\n",
    "        vocab: Vocabulary object with `encode_event()` method for converting notes to one-hot or multi-hot format\n",
    "\n",
    "    Returns:\n",
    "        encoded_sequences: List of tensors of shape [seq_length, input_dim] for notes\n",
    "        encoded_durations: List of tensors of shape [seq_length, 1] for durations\n",
    "    \"\"\"\n",
    "    if len(melody) <= seq_length:\n",
    "        raise ValueError(\"Melody too short for the given sequence length.\")\n",
    "\n",
    "    encoded_sequences = []\n",
    "    encoded_durations = []\n",
    "\n",
    "    for i in range(len(melody) - seq_length):\n",
    "        input_seq = melody[i:i+seq_length]\n",
    "        input_dur = durations[i:i+seq_length]\n",
    "\n",
    "        input_encoded = [vocab.encode_event(n) for n in input_seq]\n",
    "        encoded_sequences.append(torch.tensor(input_encoded, dtype=torch.float32))\n",
    "        encoded_durations.append(torch.tensor(input_dur, dtype=torch.float32).unsqueeze(-1))\n",
    "\n",
    "    return encoded_sequences, encoded_durations\n",
    "\n",
    "def write_midi(events, durations, filename, velocities=None, default_velocity=64):\n",
    "    \"\"\"\n",
    "    Writes a list of musical events and durations to a MIDI file.\n",
    "\n",
    "    Args:\n",
    "        events: List of notes or chords (each element is an int or a list of ints)\n",
    "        durations: List of durations in beats, aligned with events\n",
    "        filename: Output filename (e.g., \"output.mid\")\n",
    "        velocities: Optional list of velocities per event (default applied if not provided)\n",
    "        default_velocity: Default velocity to use if not specified per event\n",
    "    \"\"\"\n",
    "    mid = MidiFile()\n",
    "    track = MidiTrack()\n",
    "    mid.tracks.append(track)\n",
    "    ticks_per_beat = mid.ticks_per_beat\n",
    "\n",
    "    for i, (event, dur) in enumerate(zip(events, durations)):\n",
    "        duration_ticks = int(dur * ticks_per_beat)\n",
    "        notes = event if isinstance(event, list) else [event]\n",
    "\n",
    "        if velocities is not None:\n",
    "            velocity = velocities[i] if i < len(velocities) else default_velocity\n",
    "        else:\n",
    "            velocity = default_velocity\n",
    "\n",
    "        for note in notes:\n",
    "            track.append(Message('note_on', note=note, velocity=velocity, time=0))\n",
    "        for j, note in enumerate(notes):\n",
    "            time = duration_ticks if j == 0 else 0\n",
    "            track.append(Message('note_off', note=note, velocity=velocity, time=time))\n",
    "\n",
    "    mid.save(filename)\n",
    "    print(f\" MIDI saved: {filename}\")\n",
    "\n",
    "\n",
    "def write_midi(events, durations, filename):\n",
    "    \"\"\"\n",
    "    Converts a list of musical events and durations into a MIDI file with fixed velocity.\n",
    "    \n",
    "    Args:\n",
    "        events: List of note or chord events (each item is an int or list of ints).\n",
    "        durations: List of durations (floats, in beats) corresponding to each event.\n",
    "        filename: Output filename for the MIDI file (e.g., \"output.mid\").\n",
    "    \"\"\"\n",
    "    mid = MidiFile()\n",
    "    track = MidiTrack()\n",
    "    mid.tracks.append(track)\n",
    "    ticks_per_beat = mid.ticks_per_beat\n",
    "    velocity = 64  \n",
    "\n",
    "    for event, dur in zip(events, durations):\n",
    "        duration_ticks = int(dur * ticks_per_beat)\n",
    "        notes = event if isinstance(event, list) else [event]\n",
    "\n",
    "        for note in notes:\n",
    "            track.append(Message('note_on', note=note, velocity=velocity, time=0))\n",
    "        for i, note in enumerate(notes):\n",
    "            time = duration_ticks if i == 0 else 0\n",
    "            track.append(Message('note_off', note=note, velocity=velocity, time=time))\n",
    "\n",
    "    mid.save(filename)\n",
    "    print(f\"MIDI saved with velocity=64: {filename}\")\n",
    "\n",
    "\n",
    "def generate(encoded_sequences, encoded_durations, top_k=5, temperature=1.2):\n",
    "    \"\"\"\n",
    "    Generates a jazzified version of an input melody using a trained VAE model.\n",
    "\n",
    "    Args:\n",
    "        encoded_sequences: List of tensors containing encoded input note sequences.\n",
    "        encoded_durations: List of tensors containing corresponding duration sequences.\n",
    "        top_k: Number of top candidate notes to sample from.\n",
    "        temperature: Controls randomness; higher = more diverse output.\n",
    "\n",
    "    Returns:\n",
    "        jazzified_events: List of decoded note/chord events.\n",
    "        jazzified_durations: List of durations corresponding to each event.\n",
    "    \"\"\"\n",
    "    jazzified_events = []\n",
    "    jazzified_durations = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_seq, dur_seq in zip(encoded_sequences, encoded_durations):\n",
    "            x_seq = x_seq.unsqueeze(0).to(device)\n",
    "            dur_seq = dur_seq.unsqueeze(0).to(device)\n",
    "\n",
    "            z_mu, z_logvar = vae.encode(x_seq, dur_seq)\n",
    "            z = vae.reparameterize(z_mu, z_logvar)\n",
    "\n",
    "            recon_event, recon_dur = vae.decode(z)\n",
    "            event_vec = recon_event.squeeze().cpu().numpy()\n",
    "            duration_val = max(0.2, recon_dur.item())\n",
    "\n",
    "            logits = np.log(np.clip(event_vec, 1e-8, 1.0))  \n",
    "            scaled_logits = logits / temperature\n",
    "            probs = np.exp(scaled_logits)\n",
    "            probs /= probs.sum()\n",
    "\n",
    "            top_indices = np.argsort(probs)[-top_k:]\n",
    "            top_probs = probs[top_indices]\n",
    "            top_probs /= top_probs.sum()\n",
    "\n",
    "            note_count = np.random.choice([1, 2, 3, 4, 5], p=[0.5, 0.2, 0.2, 0.05, 0.05])\n",
    "            note_count = min(note_count, len(top_indices))\n",
    "\n",
    "            try:\n",
    "                selected_indices = np.random.choice(top_indices, size=note_count, p=top_probs, replace=False)\n",
    "            except ValueError:\n",
    "                selected_indices = [top_indices[np.argmax(top_probs)]]\n",
    "\n",
    "            decoded_event = [vocab.idx_to_note[idx] for idx in selected_indices]\n",
    "\n",
    "            jazzified_events.append(decoded_event)\n",
    "            jazzified_durations.append(duration_val)\n",
    "\n",
    "    return jazzified_events, jazzified_durations\n",
    "\n",
    "def from_song_generate (original_filename, target_filename):\n",
    "    \"\"\"\n",
    "    Loads a MIDI file, encodes it, passes it through the VAE to jazzify it,\n",
    "    and saves the generated sequence as a new MIDI file.\n",
    "\n",
    "    Args:\n",
    "        original_filename (str): Path to the input MIDI file.\n",
    "        target_filename (str): Path where the jazzified MIDI will be saved.\n",
    "    \"\"\"\n",
    "    melody, durations, velocities = parse_midi_to_event_sequence(original_filename)\n",
    "    encoded_sequences, encoded_durations = encode_input_sequence(\n",
    "        melody, durations, seq_length=16, vocab=vocab)\n",
    "    jazzified_events, jazzified_durations = generate(encoded_sequences, encoded_durations)\n",
    "    write_midi(jazzified_events, jazzified_durations, target_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating with general VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(input_dim=88, latent_dim=20, seq_length=16).to(device)\n",
    "vae.load_state_dict(torch.load('notebook/vae_model_data/vae_model.pth'))\n",
    "vae.eval()\n",
    "root='generated_music/generated_music_vae/vae_general/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sequence: [[60, 62], [64, 57, 65, 62, 60], [65, 57, 67, 62, 60], [64, 62, 63, 67, 60], [63, 65, 57, 62, 60], [65, 67, 57, 62, 60], [65, 62, 57, 64, 60], [57, 64, 67, 62, 60], [57, 65, 62, 60], [67, 57, 64, 62, 60], [57, 63, 62, 65, 60], [64, 55, 67, 62, 60], [65, 63, 59, 62, 60], [58, 62, 67, 64, 60], [62, 64, 57, 60, 65], [64, 65, 62, 60, 63], [65, 63, 57, 62, 60], [57, 67, 63, 62, 60], [63, 62, 57, 67, 60], [63, 67, 65, 60], [64, 63, 67, 60], [64, 67, 62, 63, 60], [72, 67, 65, 63, 60], [63, 57, 65, 67, 60], [63, 67, 60], [60], [63, 64, 65, 62, 60], [64, 57, 67, 62, 60], [55, 63, 67, 62, 60], [67, 65, 62, 57, 60], [58, 64, 57, 62, 60], [64, 65, 57, 62, 60]]\n",
      "Durations: [0.8988903164863586, 0.7114179730415344, 1.1524525880813599, 1.0198121070861816, 0.8861413598060608, 1.0399314165115356, 1.0773284435272217, 1.138029932975769, 0.6716316342353821, 1.2533535957336426, 0.9123578071594238, 1.117919921875, 1.1200757026672363, 0.8795191049575806, 0.9867668747901917, 1.0749266147613525, 0.8661510348320007, 0.8539105653762817, 0.9763606190681458, 0.9698623418807983, 1.054590106010437, 0.9539459943771362, 0.9715491533279419, 1.0213916301727295, 1.1300700902938843, 1.2468008995056152, 0.8490887880325317, 1.0853604078292847, 1.1229355335235596, 0.6190224289894104, 1.0833048820495605, 1.02016282081604]\n"
     ]
    }
   ],
   "source": [
    "generate_random(vae, 20, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIDI saved with velocity=64: generated_music/generated_music_vae/vae_general/HappyBirthday.mid\n",
      "MIDI saved with velocity=64: generated_music/generated_music_vae/vae_general/HipsDontLie.mid\n",
      "MIDI saved with velocity=64: generated_music/generated_music_vae/vae_general/BackInBlack.mid\n",
      "MIDI saved with velocity=64: generated_music/generated_music_vae/vae_general/FinalCountDown.mid\n"
     ]
    }
   ],
   "source": [
    "from_song_generate (\"data/popular_songs/HappyBirthday.mid\", f\"{root}HappyBirthday.mid\")\n",
    "from_song_generate (\"data/popular_songs/HipsDontLie.mid\", f\"{root}HipsDontLie.mid\")\n",
    "from_song_generate (\"data/popular_songs/BackInBlack.mid\", f\"{root}BackInBlack.mid\")\n",
    "from_song_generate (\"data/popular_songs/FinalCountDown.mid\", f\"{root}FinalCountDown.mid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "Event BCE Loss:   6.3511\n",
      "Duration MSE Loss:0.3961\n",
      "KL Divergence:    0.0085\n",
      "Total Loss:       6.7557\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6.351111392974854,\n",
       " 0.39614493429660796,\n",
       " 0.008490124642848968,\n",
       " 6.755746451914311)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(vae, val_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The event BCE loss, 6.35, dominates the total loss, reflecting the challenge of reconstructing the correct multi-hot event vector. A value around 6 is typical given the sparsity and multi-label nature of the output (e.g., chords or overlapping notes). Lower values would indicate more accurate pitch reconstruction.\n",
    "The duration mean squared error, 0.396, is relatively low, suggesting that the model predicts durations with good precision. Durations are scalar values and generally easier to regress than high-dimensional multi-hot vectors.\n",
    "The very small value of KL divergence, 0.0085, indicates that the learned posterior distribution $q(z|x)$ is already very close to the standard Gaussian prior. While this helps stabilize the latent space, excessively low KL values may imply posterior collapse, where the model underuses the latent variable $z$.\n",
    "The total loss, 6.7557 is simply the sum of the three components. The fact that the BCE term contributes ~94% of the total loss highlights that most of the model's effort is spent reconstructing pitch content, with timing and regularization playing secondary roles.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generating with VAE with KL divergence on small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sequence: [[45, 50, 48, 88, 85], [60, 91, 35, 85, 88], [52, 88, 58, 67, 80], [21, 50, 68, 85, 71], [67, 24, 68, 102, 88], [99, 21, 42, 67, 102], [71, 35, 85, 88, 50], [85, 108, 75, 45, 93], [85, 108, 83, 88, 94], [50, 35, 83, 67, 102], [85, 108, 52, 88, 30], [93, 58, 68, 52, 102], [91, 34, 35, 62, 94], [88, 85, 83, 35, 94], [58, 30, 67, 42, 85], [50, 52, 75, 96, 88], [88, 85, 45, 102, 50], [67, 88, 58, 85, 102], [49, 89, 24, 75, 88], [35, 45, 108, 88, 85], [50, 67, 108, 40, 85], [67, 50, 42, 85, 71], [85, 76, 88, 42, 50], [96, 77, 85, 62, 35], [49, 85, 102, 58, 45], [88, 96, 85, 49, 108], [62, 80, 73, 102, 38], [45, 48, 75, 50, 85], [50, 40, 108, 75, 102], [108, 89, 35, 30, 85], [85, 67, 30, 50, 88], [76, 63, 91, 88, 50]]\n",
      "Durations: [0.3971548080444336, 0.07005634903907776, 0.20556096732616425, 0.10306544601917267, 0.36613160371780396, -0.04971347004175186, 0.09934122115373611, 0.20370665192604065, 0.20289696753025055, 0.40567731857299805, 0.015450932085514069, 0.018192991614341736, 0.37254035472869873, -0.10535600781440735, 0.1606302708387375, 0.40688300132751465, 0.27291858196258545, 0.39157241582870483, 0.03532728552818298, -0.09390479326248169, 0.0766264796257019, -0.06362119317054749, 0.04476667940616608, -0.13541501760482788, 0.23107635974884033, 0.07892061024904251, 0.23821404576301575, 0.177310973405838, -0.008551105856895447, 0.11557227373123169, 0.03513254225254059, 0.2645837664604187]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/music21/stream/base.py:3675: Music21DeprecationWarning: .flat is deprecated.  Call .flatten() instead\n",
      "  return self.iter().getElementsByClass(classFilterList)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIDI saved with velocity=64: generated_music/generated_music_vae/vae_KL_small/HappyBirthday.mid\n",
      "MIDI saved with velocity=64: generated_music/generated_music_vae/vae_KL_small/HipsDontLie.mid\n",
      "MIDI saved with velocity=64: generated_music/generated_music_vae/vae_KL_small/BackInBlack.mid\n",
      "MIDI saved with velocity=64: generated_music/generated_music_vae/vae_KL_small/FinalCountDown.mid\n",
      "Evaluation Results:\n",
      "Event BCE Loss:   6.3600\n",
      "Duration MSE Loss:0.4433\n",
      "KL Divergence:    0.0085\n",
      "Total Loss:       6.8118\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6.360015144348145,\n",
       " 0.4433128082752228,\n",
       " 0.008490124642848968,\n",
       " 6.811818077266216)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_KL_small = VAE(input_dim=88, latent_dim=20, seq_length=16).to(device)\n",
    "vae_KL_small.load_state_dict(torch.load('notebook/vae_model_data/vae_model_KLdivsmalldata.pth'))\n",
    "vae_KL_small.eval()\n",
    "root = 'generated_music/generated_music_vae/vae_KL_small/'\n",
    "generate_random(vae_KL_small, 20, 16)\n",
    "from_song_generate (\"data/popular_songs/HappyBirthday.mid\", f\"{root}HappyBirthday.mid\")\n",
    "from_song_generate (\"data/popular_songs/HipsDontLie.mid\", f\"{root}HipsDontLie.mid\")\n",
    "from_song_generate (\"data/popular_songs/BackInBlack.mid\", f\"{root}BackInBlack.mid\")\n",
    "from_song_generate (\"data/popular_songs/FinalCountDown.mid\", f\"{root}FinalCountDown.mid\")\n",
    "\n",
    "evaluate_model(vae_KL_small, val_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The binary cross-entropy loss for event reconstruction, 6.3832, is relatively high. This suggests the model struggles to accurately predict multi-hot note or chord vectors, possibly due to the complexity and sparsity of the target representation.\n",
    "The mean squared error for duration prediction, 0.4448, is moderate. The model captures duration trends reasonably well but could benefit from additional tuning or structure. The KL divergence, 0.0085 is extremely low, indicating that the latent space is not being utilized effectively. This often points to posterior collapse, where the decoder ignores the latent code. The total loss, 6.8365, is driven almost entirely by the reconstruction losses, particularly the event loss, with very little contribution from KL divergence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
